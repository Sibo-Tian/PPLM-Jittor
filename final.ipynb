{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (857813129.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    export JT_SYNC=1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export JT_SYNC=1\n",
    "export trace_py_var=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1220 19:41:07.721540 00 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:07.828578 00 compiler.py:955] Jittor(1.3.5.44) src: /opt/miniconda3/envs/pplm/lib/python3.8/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:07.845401 00 compiler.py:956] clang at /usr/bin/clang++(14.0.0)\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:07.846309 00 compiler.py:957] cache_path: /Users/stian/.cache/jittor/jt1.3.5/clang14.0.0/py3.8.15/macOS-13.0.1-ax30/AppleM1Pro/default\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:08.007307 00 __init__.py:227] Total mem: 32.00GB, using 10 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:08.124684 00 jit_compiler.cc:28] Load cc_path: /usr/bin/clang++\u001b[m\n",
      "\u001b[38;5;2m[i 1220 19:41:08.134969 00 compile_extern.py:519] mpicc not found, distribution disabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "# from torchtext import data as torchtext_data\n",
    "# from torchtext import datasets\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "from operator import add\n",
    "# from run_gpt2 import top_k_logits\n",
    "import copy\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim \n",
    "\n",
    "import jittor as jt\n",
    "from jittor import nn\n",
    "from style_utils import to_var, top_k_logits\n",
    "from jittor.dataset import Dataset as jtDataset\n",
    "\n",
    "from torchtext import data as torchtext_data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "import transformers\n",
    "import random\n",
    "\n",
    "import gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClassificationHead(jt.nn.Module):\n",
    "    def __init__(self, class_size=5, embed_size=2048):\n",
    "        super().__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        self.mlp = jt.nn.Linear(embed_size, class_size)\n",
    "    def execute(self, hidden_state):\n",
    "        lm_logits = self.mlp(hidden_state)\n",
    "        return lm_logits\n",
    "\n",
    "class myDiscriminator2mean(jt.nn.Module):\n",
    "    def __init__(self, class_size=5, embed_size=1024, head=None, model=None):\n",
    "        super().__init__()\n",
    "        if head == None:\n",
    "            self.classifierhead = myClassificationHead(class_size=class_size, embed_size=embed_size)\n",
    "        else:\n",
    "            self.classifierhead = head\n",
    "        self.model = model\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def get_classifier(self):\n",
    "        return self.classifierhead\n",
    "\n",
    "    def train_custom(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        pass\n",
    "        self.classifierhead.train()\n",
    "\n",
    "    def execute(self, x):\n",
    "        mask_src = 1 - x.equal(0).unsqueeze(1).detach()\n",
    "        mask_src = mask_src.repeat(1, self.embed_size, 1) #batch_size, 1024, length (repeat each sentence for 1024 times)\n",
    "\n",
    "        x = x.tolist()\n",
    "        x = jt.array(x,dtype=torch.long)\n",
    "        output_dict = self.model(x, output_hidden_states=True)\n",
    "        hidden = output_dict.hidden_states[-1]\n",
    "        # x = model.forward_embed(x)\n",
    "        # hidden, x = model.forward_transformer_embed(x)\n",
    "        #  Hidden has shape batch_size x length x embed-dim\n",
    "        hidden = hidden.tolist()\n",
    "        hidden = jt.array(hidden)\n",
    "\n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "        _, _, batch_length = hidden.shape\n",
    "        hidden = hidden * mask_src  # / torch.sum(mask_src, dim=-1).unsqueeze(2).repeat(1, 1, batch_length)\n",
    "        #\n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "        x =  jt.sum(hidden, dim=1)/(jt.sum(mask_src, dim=-1).detach() + 1e-10)\n",
    "        x = self.classifierhead(x)\n",
    "        x = jt.nn.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmallConst = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "enc = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "config = gpt2.GPT2Config()\n",
    "m = gpt2.GPT2Model(config)\n",
    "m.load_state_dict(copy_model.transformer.state_dict())\n",
    "l = jt.nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "l.load_state_dict(copy_model.lm_head.state_dict())\n",
    "model = gpt2.GPT2LMHeadModel(config, m, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = myClassificationHead(class_size=5, embed_size=1024)\n",
    "# classifier.load_state_dict(torch.load(\"discrim_models/sentiment_classifierhead.pt\",map_location=torch.device('cpu')))\n",
    "# classifier.eval()\n",
    "# input_ids = jt.array([50256]+enc.encode('Hello guys')).astype(jt.int64).unsqueeze(dim=0)\n",
    "# prev = input_ids[:,-1:]\n",
    "\n",
    "# fuck=model(input_ids)\n",
    "# true_past = fuck.past_key_values\n",
    "# origin_prob = (fuck.logits)\n",
    "\n",
    "# fine = model(input_ids[:,:-1])\n",
    "# past = fine.past_key_values\n",
    "\n",
    "# args.loss_type=2\n",
    "# pertrubed,a,b,c = perturb_past(past=past, model=model, prev=prev, args=args, classifier=classifier, true_past=true_past, original_probs=origin_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(jt.Var([1.        3.6055512 6.4031243], dtype=float32),\n",
       " jt.Var([1.        3.6055512 6.4031243], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=jt.arange(6).reshape((3,2))\n",
    "jt.norm(p)+SmallConst,jt.norm(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_perturb(model, args, context=None, sample=True):\n",
    "    #==================================================prepare the discriminator/bag of words==============================================\n",
    "    if args.discrim == 'clickbait':\n",
    "        classifier = myClassificationHead(class_size=2, embed_size=1024)\n",
    "        classifier.load_state_dict(torch.load(\"discrim_models/clickbait_classifierhead.pt\"))\n",
    "        classifier.eval()\n",
    "        args.label_class = 1 # clickbaity\n",
    "    #TODO map_location=torch.device('cpu')\n",
    "    elif args.discrim == 'sentiment':\n",
    "        classifier = myClassificationHead(class_size=5, embed_size=1024)\n",
    "        classifier.load_state_dict(torch.load(\"discrim_models/sentiment_classifierhead.pt\",map_location=torch.device('cpu')))\n",
    "        classifier.eval()\n",
    "        if args.label_class < 0:\n",
    "            raise Exception('Wrong class for sentiment, use --label-class 2 for *very positive*, 3 for *very negative*')\n",
    "        #args.label_class = 2 # very pos\n",
    "        #args.label_class = 3 # very neg\n",
    "\n",
    "    elif args.discrim == 'toxicity':\n",
    "        classifier = myClassificationHead(class_size=2, embed_size=1024)\n",
    "        classifier.load_state_dict(torch.load(\"discrim_models/toxicity_classifierhead.pt\"))\n",
    "        classifier.eval()\n",
    "        args.label_class = 0 # not toxic\n",
    "    else:\n",
    "        classifier = None\n",
    "    \n",
    "    # Get tokens for the list of positive words\n",
    "    def list_tokens(word_list):\n",
    "        token_list = []\n",
    "        for word in word_list:\n",
    "            token_list.append(enc.encode(\" \" + word))\n",
    "        return token_list\n",
    "\n",
    "    good_index = []\n",
    "    if args.bag_of_words:\n",
    "        bags_of_words = args.bag_of_words.split(\";\")\n",
    "        for wordlist in bags_of_words:\n",
    "            with open(wordlist, \"r\") as f:\n",
    "                words = f.read()\n",
    "                words = words.split('\\n')\n",
    "            good_index.append(list_tokens(words)) # good_index is the encode of the words\n",
    "        \n",
    "    if args.bag_of_words and classifier:\n",
    "        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n",
    "        args.loss_type = 3\n",
    "\n",
    "    elif args.bag_of_words:\n",
    "        args.loss_type = 1\n",
    "        print('Using PPLM-BoW')\n",
    "\n",
    "    elif classifier is not None:\n",
    "        args.loss_type = 2\n",
    "        print('Using PPLM-Discrim')\n",
    "\n",
    "    else:\n",
    "        raise Exception('Supply either --bag-of-words (-B) or --discrim -D')\n",
    "    \n",
    "    #==================================================generate non perturbed words=======================================================\n",
    "    original, _, _ = sample_from_hidden(model=model, args=args, context=context,\n",
    "                                  perturb=False, good_index=good_index, classifier=classifier)\n",
    "\n",
    "    #==================================================generate perturbed words============================================================\n",
    "    perturbed_list = []\n",
    "    discrim_loss_list = []\n",
    "    loss_in_time_list = []\n",
    "\n",
    "    for i in range(args.num_samples): #num_samples : how many output words\n",
    "        perturbed, discrim_loss, loss_in_time = sample_from_hidden(model=model, args=args, context=context,\n",
    "                                                        perturb=True, good_index=good_index,\n",
    "                                                         classifier=classifier)\n",
    "        perturbed_list.append(perturbed)\n",
    "        if classifier is not None:\n",
    "            discrim_loss_list.append(discrim_loss)\n",
    "        loss_in_time_list.append(loss_in_time)\n",
    "    \n",
    "    return original, perturbed_list, discrim_loss_list, loss_in_time_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_hidden(model, args, classifier, context=None, past=None,\n",
    "                       sample=False, perturb=True, good_index=None):\n",
    "    \n",
    "    output = jt.int64(context).unsqueeze(0) if context else None\n",
    "    #output: 2-d list, with jt.int64 element, the format required by gpt (input_id)\n",
    "    grad_norms = None\n",
    "    loss_in_time = []\n",
    "    #iteration for args.length times, the output sentence's length = original + args.length\n",
    "    for i in trange(args.length, ascii=True):\n",
    "\n",
    "        # Get past/probs for current output, except for last word\n",
    "        # Note that GPT takes 2 inputs: past + current-token\n",
    "        # Therefore, use everything from before current i/p token to generate relevant past\n",
    "\n",
    "        if past is None and output is not None:\n",
    "            #=======================devide the output(context) into 2 parts : past and one word for query=======================\n",
    "            prev = output[:, -1:]\n",
    "            res =  model(output[:, :-1])\n",
    "            past = res['past_key_values']\n",
    "\n",
    "            res = model(output)\n",
    "            original_probs, true_past = res['logits'], res['past_key_values']\n",
    "            true_hidden = res['hidden_states'][-1]\n",
    "\n",
    "        else:\n",
    "            res = model(output)\n",
    "            original_probs, true_past = res['logits'], res['past_key_values']\n",
    "            true_hidden = res['hidden_states'][-1]\n",
    "\n",
    "        #TODO\n",
    "        if i >= args.grad_length:\n",
    "            current_stepsize = args.stepsize * 0\n",
    "        else:\n",
    "            current_stepsize = args.stepsize\n",
    "\n",
    "        #not perturb\n",
    "        if not perturb or args.num_iterations == 0:\n",
    "            perturbed_past = past\n",
    "\n",
    "        #perturb\n",
    "        else:\n",
    "        #==============================================perturb the past==============================================\n",
    "            accumulated_hidden = true_hidden[:, :-1, :] #all hidden states except query word\n",
    "            accumulated_hidden = jt.sum(accumulated_hidden, dim=1)\n",
    "            perturbed_past, _, grad_norms, loss_per_iter = perturb_past(past, model, prev, args,\n",
    "                                                                        classifier=classifier,\n",
    "                                                                        good_index=good_index, stepsize=current_stepsize,\n",
    "                                                                        original_probs=original_probs,\n",
    "                                                                        true_past=true_past,\n",
    "                                                                        accumulated_hidden=accumulated_hidden,\n",
    "                                                                        grad_norms=grad_norms)\n",
    "            loss_in_time.append(loss_per_iter)\n",
    "        #==============================================use the query word to 'query' past=============================\n",
    "        res = model(prev, past_key_values=perturbed_past)\n",
    "        test_logits, past, hidden = res['logits'], res['past_key_values'], res['hidden_states'][-1]\n",
    "         \n",
    "        # test_logits = F.softmax(test_logits[:, -1, :], dim=-1)\n",
    "        # likelywords = torch.topk(test_logits, k=10, dim=-1)\n",
    "        # print(enc.decode(likelywords[1].tolist()[0]))\n",
    "\n",
    "        if classifier is not None:\n",
    "            ce_loss = jt.nn.CrossEntropyLoss()\n",
    "            predicted_sentiment = classifier(jt.mean(true_hidden, dim=1))\n",
    "            label = jt.int64([args.label_class])\n",
    "            true_discrim_loss = ce_loss(predicted_sentiment, label)\n",
    "            print(\"true discrim loss\", true_discrim_loss)\n",
    "        else:\n",
    "            true_discrim_loss = 0 \n",
    "\n",
    "        \n",
    "        logits = test_logits\n",
    "        logits = logits[:, -1, :] / args.temperature  # + SmallConst\n",
    "\n",
    "        log_probs = jt.nn.softmax(logits, dim=-1)\n",
    "\n",
    "        # Fuse the modified model and original model\n",
    "        if perturb:\n",
    "            original_probs = jt.nn.softmax(original_probs[:, -1, :], dim=-1)\n",
    "            gm_scale = args.fusion_gm_scale\n",
    "            log_probs = ((log_probs ** gm_scale) * (original_probs ** (1 - gm_scale)))  # + SmallConst\n",
    "            log_probs = top_k_logits(log_probs, k=args.top_k, probs=True)  # + SmallConst\n",
    "\n",
    "            if jt.sum(log_probs) <= 1:\n",
    "                log_probs = log_probs / jt.sum(log_probs)\n",
    "        \n",
    "        else:\n",
    "            # logits = top_k_logits(logits, k=args.top_k)  # + SmallConst\n",
    "            log_probs = jt.nn.softmax(logits, dim=-1)\n",
    "            \n",
    "        #==============================================from [50256] -> [1]==============================================\n",
    "        if sample:\n",
    "            # likelywords = jt.topk(log_probs, k=args.top_k, dim=-1)\n",
    "            # print(enc.decode(likelywords[1].tolist()[0]))\n",
    "            # print(likelywords[0].tolist())\n",
    "            # np.random.choice()\n",
    "            #TODO\n",
    "            prev = jt.multinomial(log_probs, num_samples=1)\n",
    "        else:\n",
    "            _, prev = jt.topk(log_probs, k=1, dim=-1)\n",
    "\n",
    "        output = prev if output is None else jt.concat((output, prev), dim=1)  # update output\n",
    "        print(enc.decode(output.tolist()[0]))\n",
    "\n",
    "    return output, true_discrim_loss, loss_in_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_past(past, model, prev, args, classifier, true_past, original_probs,stepsize=0.01, vocab_size=50257, \n",
    "                good_index=None,accumulated_hidden=None,  grad_norms=None):\n",
    "    #prev: 2d jt.array\n",
    "    #past: tuple(tuple(tuple(2d jt.array)))\n",
    "    #==============================================use prev to query past in model==============================================\n",
    "    \n",
    "\n",
    "    gm_scale, kl_scale = args.fusion_gm_scale, args.fusion_kl_scale\n",
    "    one_hot_vectors = []\n",
    "    if good_index is not None:\n",
    "        for good_list in good_index:\n",
    "            good_list = list(filter(lambda x: len(x) <= 1, good_list))\n",
    "            good_list = jt.array(good_list)\n",
    "            num_good = good_list.shape[0]\n",
    "            one_hot_good = jt.zeros(num_good, vocab_size)\n",
    "            one_hot_good.scatter_(1, good_list, 1)\n",
    "            one_hot_vectors.append(one_hot_good)\n",
    "\n",
    "    # Generate inital perturbed past, we accumulate grad on this\n",
    "    past_perturb_orig = []\n",
    "    for layer in past:\n",
    "        past_perturb_orig.append([jt.zeros_like(x).astype(jt.float32) for x in layer])\n",
    "\n",
    "    if accumulated_hidden is None:\n",
    "        accumulated_hidden = 0\n",
    "\n",
    "    # ==============================================Generate a mask is gradient perturbated is based on a past window=============================================\n",
    "    # window mask is all you need: it combines decay mask and window_length(naive window mask)\n",
    "    window_length = args.window_length\n",
    "    _, _, current_length, _ = past[0][0].shape\n",
    "    #mask-part1-decay mask\n",
    "    if args.decay:\n",
    "        decay_mask = jt.arange(0., 1.0 + SmallConst, 1.0/(window_length))[1:]\n",
    "    else:\n",
    "        decay_mask = 1.0\n",
    "    #mask-part2\n",
    "    if current_length > window_length and window_length > 0:\n",
    "        ones_key_val_shape = tuple(past[0][0].shape[:-2]) + tuple([window_length]) + tuple(\n",
    "            past[0][0].shape[-1:]) #(batch_size, num_heads) + (seq_length) + (n_embd)\n",
    "\n",
    "        zeros_key_val_shape = tuple(past[0][0].shape[:-2]) + tuple([current_length - window_length]) + tuple(\n",
    "            past[0][0].shape[-1:])\n",
    "\n",
    "        ones_mask = jt.ones(ones_key_val_shape)\n",
    "        ones_mask = decay_mask*ones_mask.permute(0, 1, 3, 2)\n",
    "        ones_mask = ones_mask.permute(0, 1, 3, 2)\n",
    "\n",
    "        window_mask = jt.concat((ones_mask, jt.zeros(zeros_key_val_shape)), dim=-2)\n",
    "    else:\n",
    "        window_mask = jt.ones_like(past[0][0])\n",
    "    #====================================perturb the past for args.num_iteration times, which is similar to do 'optimizer step' for several times=================\n",
    "    def key_values_add(x,y):\n",
    "            return list(map(add, x,y))\n",
    "    loss_per_iter = []\n",
    "    for i in range(args.num_iterations):\n",
    "        perturbed_past = list(map(key_values_add, past, past_perturb_orig))\n",
    "        _, _, current_length, _ = past_perturb_orig[0][0].shape\n",
    "        # Compute hidden using perturbed past\n",
    "        result = model(prev, past_key_values=perturbed_past)\n",
    "        hidden = result['hidden_states'][-1] #(batch_size, 1, 1024)\n",
    "        new_accumulated_hidden = accumulated_hidden + jt.sum(hidden, dim=1) #(batch_size, 1024)\n",
    "\n",
    "        # TODO: Check the layer-norm consistency of this with trained discriminator\n",
    "        #TODO\n",
    "        logits = result['logits']\n",
    "        logits = logits[:, -1, :]\n",
    "        probabs = jt.nn.softmax(logits, dim=-1)\n",
    "        #TODO\n",
    "        #========================================================================calculate the loss================================================================\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        #bag of words\n",
    "        if args.loss_type == 1 or args.loss_type == 3:\n",
    "            for one_hot_good in one_hot_vectors:\n",
    "                good_logits = jt.matmul(probabs, jt.transpose(one_hot_good))\n",
    "                loss_word = good_logits\n",
    "                loss_word = jt.sum(loss_word)\n",
    "                loss_word = -jt.log(loss_word)\n",
    "                #loss_word = torch.sum(loss_word) /torch.sum(one_hot_good)\n",
    "                loss += loss_word\n",
    "                loss_list.append(loss_word)\n",
    "            print('words', loss)\n",
    "\n",
    "        if args.loss_type == 2 or args.loss_type == 3:\n",
    "            new_true_past = true_past\n",
    "            #after using prev to query past, then do args.horizon_length times query, to better calculate the discriminator loss\n",
    "            for i in range(args.horizon_length):\n",
    "\n",
    "                future_probabs = jt.nn.softmax(logits, dim=-1)  # Get softmax\n",
    "                future_probabs = jt.unsqueeze(future_probabs, dim=1)\n",
    "                _,future_input_id = jt.topk(future_probabs, k=1, dim=-1)\n",
    "\n",
    "                res= model(future_input_id, past_key_values=new_true_past)\n",
    "                new_true_past = res['past_key_values']\n",
    "                future_hidden = res['hidden_states'][-1]  # Get expected hidden states\n",
    "                new_accumulated_hidden = new_accumulated_hidden + jt.sum(future_hidden, dim=1)\n",
    "                \n",
    "            predicted_sentiment = classifier(new_accumulated_hidden / (current_length + 1 + args.horizon_length))\n",
    "\n",
    "            label = jt.array([args.label_class for i in range(predicted_sentiment.shape[0])], dtype=jt.int64)\n",
    "            discrim_loss = nn.cross_entropy_loss(predicted_sentiment, label)\n",
    "            print('discrim', discrim_loss)\n",
    "            loss += discrim_loss\n",
    "            loss_list.append(discrim_loss)\n",
    "\n",
    "\n",
    "        kl_loss = 0.0\n",
    "        if kl_scale > 0.0:\n",
    "            p = (jt.nn.softmax(original_probs[:, -1:, :], dim=-1)).squeeze(dim=1) #(batch_size, 50256)\n",
    "            #TODO correct\n",
    "            p = p + SmallConst * (p <= SmallConst)\n",
    "            correction = SmallConst * (probabs <= SmallConst)\n",
    "            corrected_probabs = probabs + correction\n",
    "            kl_loss = kl_scale * ((corrected_probabs * jt.log(corrected_probabs / p)).sum())\n",
    "            loss += kl_loss\n",
    "\n",
    "        print('Discrim Loss: ',(loss - kl_loss)*10000)\n",
    "        loss_per_iter.append(loss)\n",
    "        #========================================================================get the grad of past========================================================================\n",
    "        grad = []\n",
    "        for layer in perturbed_past:\n",
    "            grad.append([jt.grad(loss, block) for block in layer])\n",
    "\n",
    "        if grad_norms is not None and args.loss_type == 1:\n",
    "            grad_norms = [jt.max(grad_norms[index], jt.norm(p_.grad * window_mask)) for index, p_ in\n",
    "                          enumerate(past_perturb_orig)]\n",
    "        else:\n",
    "            grad_norms = []\n",
    "            for layer in enumerate(grad):\n",
    "                # grad_norms.append([(jt.norm(block * window_mask) + SmallConst) for block in layer])\n",
    "                #TODO\n",
    "                grad_norms.append([1 for block in layer])\n",
    "\n",
    "        perturb_grad = []\n",
    "        for i, layer in enumerate(past_perturb_orig):\n",
    "            perturb_grad.append([-stepsize * (grad[i][j] * window_mask / grad_norms[i][j] ** args.gamma) for j, _ in enumerate(layer)])\n",
    "        \n",
    "        past_perturb_orig = list(map(key_values_add, perturb_grad, past_perturb_orig))\n",
    "\n",
    "    \n",
    "    perturbed_past = list(map(key_values_add, past, past_perturb_orig))\n",
    "    return perturbed_past, new_accumulated_hidden, grad_norms, loss_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Bags of words used for PPLM-BoW. Multiple BoWs separated by ;\n",
    "parser.add_argument('--bag-of-words', '-B', type=str, default=None, \n",
    "                    help='')\n",
    "# Discriminator to use for loss-type 2\n",
    "parser.add_argument('--discrim', '-D', type=str, default='sentiment', \n",
    "                    choices=('clickbait', 'sentiment', 'toxicity'), \n",
    "                    help='')\n",
    "parser.add_argument('--label-class', type=int, default=2, help='Class label used for the discriminator')\n",
    "parser.add_argument('--stepsize', type=float, default=0.02)\n",
    "parser.add_argument(\"--length\", type=int, default=5)\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "# top-k采样\n",
    "parser.add_argument(\"--top_k\", type=int, default=10)\n",
    "# \n",
    "parser.add_argument(\"--fusion-gm-scale\", type=float, default=0.9)\n",
    "parser.add_argument(\"--fusion-kl-scale\", type=float, default=0.01)\n",
    "parser.add_argument('--nocuda', action='store_true', help='no cuda')\n",
    "# Generate from end-of-text as prefix\n",
    "parser.add_argument('--uncond', action='store_true', help='前缀为end-of-text')\n",
    "parser.add_argument(\"--cond-text\", type=str, default='The lake', help='Prefix texts to condition on')\n",
    "parser.add_argument('--num-iterations', type=int, default=3)\n",
    "parser.add_argument('--grad-length', type=int, default=10000)\n",
    "parser.add_argument('--num-samples', type=int, default=1,\n",
    "                    help='Number of samples to generate from the modified latents')\n",
    "parser.add_argument('--horizon-length', type=int, default=1, help='Length of future to optimize over')\n",
    "# parser.add_argument('--force-token', action='store_true', help='no cuda')\n",
    "parser.add_argument('--window-length', type=int, default=0,\n",
    "                    help='Length of past which is being optimizer; 0 corresponds to infinite window length')\n",
    "parser.add_argument('--decay', action='store_true', help='whether to decay or not')\n",
    "parser.add_argument('--gamma', type=float, default=1.5)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# 设置随机种子\n",
    "np.random.seed(args.seed)\n",
    "jt.core.set_seed(args.seed)\n",
    "\n",
    "# use cuda\n",
    "if not args.nocuda: \n",
    "    jt.flags.use_cuda = 0\n",
    "\n",
    "# load_pretrained\n",
    "if args.uncond:\n",
    "    seq = [[50256, 50256]]\n",
    "\n",
    "else:\n",
    "    # 前缀词\n",
    "    raw_text = args.cond_text\n",
    "    while not raw_text:\n",
    "        print('Did you forget to add `--cond-text`? ')\n",
    "        raw_text = input(\"Model prompt >>> \")\n",
    "    seq = [[50256] + enc.encode(raw_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Prefix of sentence ========================================\n",
      "<|endoftext|>The lake\n",
      "================================================================================\n",
      "Using PPLM-Discrim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|####      | 2/5 [00:04<00:06,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jt.Var([5.9967017], dtype=float32)\n",
      "<|endoftext|>The lake is\n",
      "true discrim loss jt.Var([4.5345993], dtype=float32)\n",
      "<|endoftext|>The lake is a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|########  | 4/5 [00:07<00:01,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss jt.Var([3.6155896], dtype=float32)\n",
      "<|endoftext|>The lake is a popular\n",
      "true discrim loss jt.Var([2.0310307], dtype=float32)\n",
      "<|endoftext|>The lake is a popular destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5/5 [00:08<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss jt.Var([1.4047372], dtype=float32)\n",
      "<|endoftext|>The lake is a popular destination for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|##        | 1/5 [00:40<02:41, 40.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss jt.Var([5.996702], dtype=float32)\n",
      "<|endoftext|>The lake is\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|####      | 2/5 [01:18<01:57, 39.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss jt.Var([4.5346026], dtype=float32)\n",
      "<|endoftext|>The lake is a\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|######    | 3/5 [01:57<01:17, 38.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true discrim loss jt.Var([3.615591], dtype=float32)\n",
      "<|endoftext|>The lake is a popular\n",
      "discrim jt.Var([0.], dtype=float32)\n",
      "Discrim Loss:  jt.Var([0.], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|######    | 3/5 [02:08<01:25, 42.87s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\u001b[38;5;1m[f 1220 19:43:37.188516 00 executor.cc:666] \nExecute fused operator(183/371) failed. \n[JIT Source]: /Users/stian/.cache/jittor/jt1.3.5/clang14.0.0/py3.8.15/macOS-13.0.1-ax30/AppleM1Pro/default/jit/__opkey0_broadcast_to__Tx_float32__DIM_5__BCAST_10__opkey1_broadcast_to__Tx_float32__DIM_5___hash_3cd5b6b707a85aa6_op.cc \n[OP TYPE]: fused_op:( broadcast_to, broadcast_to, binary.multiply, reduce.add,)\n[Input]: float32[1,16,1,6,], float32[1,16,1,64,], \n[Output]: float32[1,16,6,64,], \n[Async Backtrace]: not found, please set env JT_SYNC=1, trace_py_var=3 \n[Reason]: \u001b[38;5;1m[f 1220 19:43:37.188431 00 sfrl_allocator.cc:70] Check failed: tot_block_id < ID_LIMIT - 1  Something wrong... Could you please report this issue?\n block id limit extended.\u001b[m\u001b[m\n**********\nAsync error was detected. To locate the async backtrace and get better error report, please rerun your code with two enviroment variables set:\n>>> export JT_SYNC=1\n>>> export trace_py_var=3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m out1, out_perturb, discrim_loss_list, loss_in_time_list \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_perturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m text_whole \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mdecode(out1\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn [11], line 67\u001b[0m, in \u001b[0;36mlatent_perturb\u001b[0;34m(model, args, context, sample)\u001b[0m\n\u001b[1;32m     64\u001b[0m loss_in_time_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mnum_samples): \u001b[38;5;66;03m#num_samples : how many output words\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     perturbed, discrim_loss, loss_in_time \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgood_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     perturbed_list\u001b[38;5;241m.\u001b[39mappend(perturbed)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m classifier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [12], line 45\u001b[0m, in \u001b[0;36msample_from_hidden\u001b[0;34m(model, args, classifier, context, past, sample, perturb, good_index)\u001b[0m\n\u001b[1;32m     43\u001b[0m     accumulated_hidden \u001b[38;5;241m=\u001b[39m true_hidden[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m#all hidden states except query word\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     accumulated_hidden \u001b[38;5;241m=\u001b[39m jt\u001b[38;5;241m.\u001b[39msum(accumulated_hidden, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     perturbed_past, _, grad_norms, loss_per_iter \u001b[38;5;241m=\u001b[39m \u001b[43mperturb_past\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mgood_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgood_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_stepsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43moriginal_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mtrue_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43maccumulated_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccumulated_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mgrad_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_norms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     loss_in_time\u001b[38;5;241m.\u001b[39mappend(loss_per_iter)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#==============================================use the query word to 'query' past=============================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [13], line 122\u001b[0m, in \u001b[0;36mperturb_past\u001b[0;34m(past, model, prev, args, classifier, true_past, original_probs, stepsize, vocab_size, good_index, accumulated_hidden, grad_norms)\u001b[0m\n\u001b[1;32m    120\u001b[0m grad \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m perturbed_past:\n\u001b[0;32m--> 122\u001b[0m     grad\u001b[38;5;241m.\u001b[39mappend([jt\u001b[38;5;241m.\u001b[39mgrad(loss, block) \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m layer])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_norms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    125\u001b[0m     grad_norms \u001b[38;5;241m=\u001b[39m [jt\u001b[38;5;241m.\u001b[39mmax(grad_norms[index], jt\u001b[38;5;241m.\u001b[39mnorm(p_\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m window_mask)) \u001b[38;5;28;01mfor\u001b[39;00m index, p_ \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    126\u001b[0m                   \u001b[38;5;28menumerate\u001b[39m(past_perturb_orig)]\n",
      "Cell \u001b[0;32mIn [13], line 122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m grad \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m perturbed_past:\n\u001b[0;32m--> 122\u001b[0m     grad\u001b[38;5;241m.\u001b[39mappend([\u001b[43mjt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m layer])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_norms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    125\u001b[0m     grad_norms \u001b[38;5;241m=\u001b[39m [jt\u001b[38;5;241m.\u001b[39mmax(grad_norms[index], jt\u001b[38;5;241m.\u001b[39mnorm(p_\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m window_mask)) \u001b[38;5;28;01mfor\u001b[39;00m index, p_ \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    126\u001b[0m                   \u001b[38;5;28menumerate\u001b[39m(past_perturb_orig)]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pplm/lib/python3.8/site-packages/jittor/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(loss, targets, retain_graph)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad\u001b[39m(loss, targets, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(targets) \u001b[39m==\u001b[39m core\u001b[39m.\u001b[39mVar:\n\u001b[0;32m--> 436\u001b[0m         \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39;49mgrad(loss, [targets], retain_graph)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39mgrad(loss, targets, retain_graph)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \u001b[38;5;1m[f 1220 19:43:37.188516 00 executor.cc:666] \nExecute fused operator(183/371) failed. \n[JIT Source]: /Users/stian/.cache/jittor/jt1.3.5/clang14.0.0/py3.8.15/macOS-13.0.1-ax30/AppleM1Pro/default/jit/__opkey0_broadcast_to__Tx_float32__DIM_5__BCAST_10__opkey1_broadcast_to__Tx_float32__DIM_5___hash_3cd5b6b707a85aa6_op.cc \n[OP TYPE]: fused_op:( broadcast_to, broadcast_to, binary.multiply, reduce.add,)\n[Input]: float32[1,16,1,6,], float32[1,16,1,64,], \n[Output]: float32[1,16,6,64,], \n[Async Backtrace]: not found, please set env JT_SYNC=1, trace_py_var=3 \n[Reason]: \u001b[38;5;1m[f 1220 19:43:37.188431 00 sfrl_allocator.cc:70] Check failed: tot_block_id < ID_LIMIT - 1  Something wrong... Could you please report this issue?\n block id limit extended.\u001b[m\u001b[m\n**********\nAsync error was detected. To locate the async backtrace and get better error report, please rerun your code with two enviroment variables set:\n>>> export JT_SYNC=1\n>>> export trace_py_var=3\n"
     ]
    }
   ],
   "source": [
    "collect_gen = dict()\n",
    "current_index = 0 \n",
    "for out in seq:\n",
    "\n",
    "    text = enc.decode(out)\n",
    "    print(\"=\" * 40 + \" Prefix of sentence \" + \"=\" * 40)\n",
    "    print(text)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    out1, out_perturb, discrim_loss_list, loss_in_time_list = latent_perturb(model=model, args=args, context=out)\n",
    "\n",
    "    text_whole = enc.decode(out1.tolist()[0])\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 40 + \" Whole sentence (Original)\" + \"=\" * 40)\n",
    "    print(text_whole)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    out_perturb_copy = out_perturb\n",
    "\n",
    "    generated = 0\n",
    "    # 干扰后的结果\n",
    "    for out_perturb in out_perturb_copy:\n",
    "        try:\n",
    "            print(\"=\" * 40 + \" Whole sentence (Perturbed)\" + \"=\" * 40)\n",
    "            text_whole = enc.decode(out_perturb.tolist()[0])\n",
    "            print(text_whole)\n",
    "            print(\"=\" * 80)\n",
    "        except:\n",
    "            pass\n",
    "        collect_gen[current_index] = [out, out_perturb, out1]\n",
    "        # Save the prefix, perturbed seq, original seq for each index\n",
    "\n",
    "        current_index = current_index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54f0cfbc081d2dbd9b32bca22e677f6cddd072e682c5c874abb322034cb62c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
