{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import trange\n",
    "import datasets\n",
    "import jittor as jt\n",
    "import gpt2\n",
    "import time\n",
    "import transformers\n",
    "from classification import ClassificationHead, Discriminator2mean\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "config = gpt2.GPT2Config()\n",
    "model = gpt2.GPT2LMHeadModel(config)\n",
    "model.load('gpt2-medium.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(jt.dataset.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x #2d\n",
    "        self.y = jt.array(y) #1d\n",
    "        lengths = [len(seq) for seq in x]\n",
    "        padding = jt.zeros(len(lengths), max(lengths))\n",
    "        for i, seq in enumerate(x):\n",
    "            padding[i,:lengths[i]] = seq[:lengths[i]]\n",
    "        self.x = padding\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        return self.x[k], self.y[k]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "class cacheDataset(jt.dataset.Dataset):\n",
    "    def __init__(self, x, y, model):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = jt.array(y) #1d\n",
    "        lengths = [len(seq) for seq in x]\n",
    "        padding = jt.zeros(len(lengths), max(lengths))\n",
    "        for i, seq in enumerate(x):\n",
    "            padding[i,:lengths[i]] = seq[:lengths[i]]\n",
    "        self.x = padding\n",
    "        #cache\n",
    "        mask_src = 1 - x.equal(0).unsqueeze(1).detach()\n",
    "        mask_src = mask_src.repeat(1, self.embed_size, 1)\n",
    "\n",
    "        result = model(self.x)\n",
    "        hidden = result['hidden_states'][-1]\n",
    "\n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "        hidden = hidden * mask_src  \n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "\n",
    "        self.x =  jt.sum(hidden, dim=1)/(jt.sum(mask_src, dim=-1) + 1e-10)\n",
    "    def __getitem__(self, k):\n",
    "        return self.x[k], self.y[k]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, discriminator:Discriminator2mean, args=None):\n",
    "    optimizer = jt.optim.Adam(discriminator.get_classifier_param(), lr=args.lr)\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            print('Epoch: {}, batch: {}'.format(epoch,batch_idx))\n",
    "            start = time.time()\n",
    "            data, target = data, target.reshape(-1) # data is 2-d list [batch_size, length(after padding)], target is 1-d list [batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            output = discriminator(data)\n",
    "            loss = jt.nn.nll_loss(output, target)\n",
    "            optimizer.step(loss)\n",
    "            print('batch time cost: {}'.format(time.time() - start))\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Relu Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(data_loader),\n",
    "                        batch_idx * len(data) / len(data_loader), loss.item()))\n",
    "        head = discriminator.get_classifier()\n",
    "        head.save(args.dataset_label+'-'+str(epoch)+'.pkl')\n",
    "\n",
    "        \n",
    "def train_cache(data_loader, classificationHead, args):\n",
    "    optimizer = jt.optim.Adam(classificationHead.parameters(), lr=args.lr)\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            print('Epoch: {}, batch: {}'.format(epoch,batch_idx))\n",
    "            start = time.time()\n",
    "            data, target = data, target.reshape(-1) # data is 2-d list [batch_size, length(after padding)], target is 1-d list [batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            output = classificationHead(data)\n",
    "            loss = jt.nn.nll_loss(output, target)\n",
    "            optimizer.step(loss)\n",
    "            print('batch time cost: {}'.format(time.time() - start))\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Relu Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(data_loader),\n",
    "                        batch_idx * len(data) / len(data_loader), loss.item()))\n",
    "        classificationHead.save(args.dataset_label+'-'+str(epoch)+'.pkl')\n",
    "\n",
    "    \n",
    "def test_epoch(data_loader, discriminator, args=None):\n",
    "    discriminator.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with jt.no_grad():\n",
    "        idx = 1\n",
    "        for data, target in data_loader:\n",
    "            print('batch:{} / total{}'.format(idx, len(data_loader)))\n",
    "            idx += 1\n",
    "            output = discriminator(data)\n",
    "            test_loss += jt.nn.nll_loss(output, target.reshape(-1)).item()  # sum up batch loss\n",
    "            pred,_ = output.argmax(dim=1, keepdims=True)  # get the index of the max log-probability\n",
    "            correct += pred.equal(target.reshape(pred.shape)).sum().item()\n",
    "\n",
    "    print('\\nRelu Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader),\n",
    "        100. * correct / len(data_loader)))\n",
    "    positive_example = 'I love you'\n",
    "    negative_example = 'I hate you'\n",
    "    positive_encoded = jt.array(tokenizer.encode(positive_example)).unsqueeze(dim=0)\n",
    "    negative_encoded = jt.array(tokenizer.encode(negative_example)).unsqueeze(dim=0)\n",
    "    positive_res = discriminator(positive_encoded).squeeze(dim=0)\n",
    "    negative_res = discriminator(negative_encoded).squeeze(dim=0)\n",
    "    print(positive_example,' :','\"positive\":{}, \"negative\": {}, \"very positive\": {}, \"very negative\": {}, \"neutral\": {} \\n'.format(positive_res[0],\n",
    "                                                                                positive_res[1],positive_res[2],positive_res[3],positive_res[4]))\n",
    "    print(negative_example,' :','\"positive\":{}, \"negative\": {}, \"very positive\": {}, \"very negative\": {}, \"neutral\": {} \\n'.format(negative_res[0],\n",
    "                                                                                negative_res[1],negative_res[2],negative_res[3],negative_res[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jt.flags.use_cuda = jt.has_cuda\n",
    "parser = argparse.ArgumentParser(description='Train a discriminator on top of GPT-2 representations')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='Number of training epochs')\n",
    "parser.add_argument('--lr',type=float, default=0.0001)\n",
    "parser.add_argument('--save_path', type=str, default='', help='whether to save the model')\n",
    "parser.add_argument('--dataset_label', type=str, default='SST')\n",
    "parser.add_argument('--cache_hidden',action='store_true')\n",
    "args = parser.parse_args(args=['--cache_hidden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_label == 'SST':\n",
    "    raw_train_dataset = datasets.load_dataset('sst2', split='train')\n",
    "    raw_test_dataset = datasets.load_dataset('sst2', split='validation')#use validation dataset, as no labels for test dataset\n",
    "    x = [[50256]+tokenizer.encode(sen) for sen in raw_train_dataset['sentence']]\n",
    "    y = raw_train_dataset['label']\n",
    "    if not args.cache_hidden:\n",
    "        train_dataset = myDataset(x,y).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "    else:\n",
    "        train_dataset = cacheDataset(x,y,model).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "    x = [[50256]+tokenizer.encode(sen) for sen in raw_test_dataset['sentence']]\n",
    "    y = raw_test_dataset['label']\n",
    "    if not args.cache_hidden:\n",
    "        test_dataset = myDataset(x,y).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "    else:\n",
    "        test_dataset = cacheDataset(x,y,model).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "else:\n",
    "    raise Exception('Not support for this dataset...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator2mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(train_dataset, model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "54f0cfbc081d2dbd9b32bca22e677f6cddd072e682c5c874abb322034cb62c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
