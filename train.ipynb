{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import trange\n",
    "# from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext import data as torchtext_data\n",
    "from torchtext import datasets\n",
    "\n",
    "import jittor as jt\n",
    "import gpt2\n",
    "import time\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(jt.dataset.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x #2d\n",
    "        self.y = jt.array(y) #1d\n",
    "        lengths = [len(seq) for seq in x]\n",
    "        padding = jt.zeros(len(lengths), max(lengths))\n",
    "        for i, seq in enumerate(x):\n",
    "            padding[i,:lengths[i]] = seq[:lengths[i]]\n",
    "        self.x = padding\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        return self.x[k], self.y[k]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "# model = transformers.GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(jt.nn.Module):\n",
    "    def __init__(self, class_size=5, embed_size=2048):\n",
    "        super().__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        self.mlp = jt.nn.Linear(embed_size, class_size)\n",
    "    def execute(self, hidden_state):\n",
    "        lm_logits = self.mlp(hidden_state)\n",
    "        return lm_logits\n",
    "\n",
    "class Discriminator2mean(jt.nn.Module):\n",
    "    def __init__(self, class_size=5, embed_size=1024, head=None):\n",
    "        super().__init__()\n",
    "        if head == None:\n",
    "            self.classifierhead = ClassificationHead(class_size=class_size, embed_size=embed_size)\n",
    "        else:\n",
    "            self.classifierhead = head\n",
    "        config = gpt2.GPT2Config()\n",
    "        self.model = gpt2.GPT2LMHeadModel(config)\n",
    "        self.model.load('gpt2.pkl')\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def get_classifier(self):\n",
    "        return self.classifierhead\n",
    "\n",
    "    def get_classifier_param(self):\n",
    "        return self.classifierhead.parameters()\n",
    "\n",
    "    def execute(self, x):\n",
    "        mask_src = 1 - x.equal(0).unsqueeze(1).detach()\n",
    "        mask_src = mask_src.repeat(1, self.embed_size, 1) #batch_size, 1024, length (repeat each sentence for 1024 times)\n",
    "\n",
    "        output_dict = self.model(x)\n",
    "        hidden = output_dict.hidden_states[-1]\n",
    "\n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "        hidden = hidden * mask_src  # / torch.sum(mask_src, dim=-1).unsqueeze(2).repeat(1, 1, batch_length)\n",
    "        #\n",
    "        hidden = hidden.permute(0, 2, 1)\n",
    "        x =  jt.sum(hidden, dim=1)/(jt.sum(mask_src, dim=-1) + 1e-10)\n",
    "        x = self.classifierhead(x)\n",
    "        x = jt.nn.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jt.flags.use_cuda = jt.has_cuda\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train a discriminator on top of GPT-2 representations')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='Number of training epochs')\n",
    "parser.add_argument('--save_path', type=str, default='', help='whether to save the model')\n",
    "parser.add_argument('--load_path', type=str, default='')\n",
    "parser.add_argument('--dataset_label', type=str, default='SST',choices=('SST', 'clickbait', 'toxic'))\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_label == 'SST':\n",
    "    text = torchtext_data.Field()\n",
    "    label = torchtext_data.Field(sequential=False)\n",
    "    train_data, val_data, test_data = datasets.SST.splits(text, label, fine_grained=True, train_subtrees=True,\n",
    "                                                            # filter_pred=lambda ex: ex.label != 'neutral'\n",
    "                                                            )\n",
    "    d = {\"positive\": 0, \"negative\": 1, \"very positive\": 2, \"very negative\": 3, \"neutral\": 4}\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(train_data)):\n",
    "        seq = TreebankWordDetokenizer().detokenize(vars(train_data[i])[\"text\"])\n",
    "        seq = tokenizer.encode(seq)\n",
    "        x.append(seq)\n",
    "        y.append(d[vars(train_data[i])[\"label\"]])\n",
    "    train_dataset = myDataset(x,y).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(len(test_data)):\n",
    "        seq = TreebankWordDetokenizer().detokenize(vars(test_data[i])[\"text\"])\n",
    "        seq = tokenizer.encode(seq)\n",
    "        seq = [50256] + seq\n",
    "        #seq = torch.tensor([50256] + seq, device=device, dtype=torch.long)\n",
    "        test_x.append(seq)\n",
    "        test_y.append(d[vars(test_data[i])[\"label\"]])\n",
    "    test_dataset = myDataset(test_x, test_y).set_attrs(batch_size=args.batch_size,shuffle=True)\n",
    "elif args.dataset_label == 'clickbait':\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, discriminator:Discriminator2mean, args=None):\n",
    "    optimizer = jt.optim.Adam(discriminator.get_classifier_param(), lr=0.0001)\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            print('Epoch: {}, batch: {}'.format(idx,batch_idx))\n",
    "            start = time.time()\n",
    "            data, target = data, target.reshape(-1) # data is 2-d list [batch_size, length(after padding)], target is 1-d list [batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            output = discriminator(data)\n",
    "            loss = jt.nn.nll_loss(output, target)\n",
    "            optimizer.step(loss)\n",
    "            print('batch time cost: {}'.format(time.time() - start))\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Relu Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(data_loader),\n",
    "                        batch_idx * len(data) / len(data_loader), loss.item()))\n",
    "        head = discriminator.get_classifier()\n",
    "        head.save(args.dataset_label+'-'+str(epoch)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(data_loader, discriminator, args=None):\n",
    "    discriminator.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with jt.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = discriminator(data)\n",
    "            test_loss += jt.nn.nll_loss(output, target.reshape(-1)).item()  # sum up batch loss\n",
    "            pred,_ = output.argmax(dim=1, keepdims=True)  # get the index of the max log-probability\n",
    "            correct += pred.equal(target.reshape(pred.shape)).sum().item()\n",
    "\n",
    "    print('\\nRelu Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader),\n",
    "        100. * correct / len(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator2mean()\n",
    "train_epoch(train_dataset, model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54f0cfbc081d2dbd9b32bca22e677f6cddd072e682c5c874abb322034cb62c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
