{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0112 00:59:40.053276 00 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:40.135973 00 compiler.py:955] Jittor(1.3.6.10) src: /opt/miniconda3/envs/pplm/lib/python3.8/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:40.157407 00 compiler.py:956] clang at /usr/bin/clang++(14.0.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:40.158287 00 compiler.py:957] cache_path: /Users/stian/.cache/jittor/jt1.3.6/clang14.0.0/py3.8.15/macOS-13.0.1-ax30/AppleM1Pro/default\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:40.337787 00 __init__.py:227] Total mem: 32.00GB, using 10 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:40.535421 00 jit_compiler.cc:28] Load cc_path: /usr/bin/clang++\u001b[m\n",
      "\u001b[38;5;2m[i 0112 00:59:41.315762 00 compile_extern.py:522] mpicc not found, distribution disabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import trange\n",
    "from operator import add\n",
    "import transformers\n",
    "import jittor as jt\n",
    "import gpt2\n",
    "from style_utils import top_k_logits, dist_n\n",
    "from classification import ClassificationHead, Discriminator2mean\n",
    "from evaluate import load\n",
    "\n",
    "SmallConst = 1e-15\n",
    "enc = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_past(past, model, prev, args, classifier, true_past, original_probs,stepsize=0.01, vocab_size=50257, \n",
    "                good_index=None,accumulated_hidden=None,  grad_norms=None):\n",
    "    gm_scale, kl_scale = args.fusion_gm_scale, args.fusion_kl_scale\n",
    "    one_hot_vectors = []\n",
    "    if good_index is not None:\n",
    "        for good_list in good_index:\n",
    "            good_list = jt.array(good_list)\n",
    "            num_good = good_list.shape[0]\n",
    "            one_hot_good = jt.zeros(num_good, vocab_size)\n",
    "            one_hot_good.scatter_(1, good_list, jt.ones(num_good, 1))\n",
    "            one_hot_vectors.append(one_hot_good)\n",
    "    #Generate inital perturbed past, we accumulate grad on this\n",
    "    accu_grad = []\n",
    "    for layer in past:\n",
    "        accu_grad.append([jt.zeros_like(x).astype(jt.float32) for x in layer])\n",
    "\n",
    "    if accumulated_hidden is None:\n",
    "        accumulated_hidden = 0\n",
    "\n",
    "    # ==============================================Generate a mask is gradient perturbated is based on a past window=============================================\n",
    "    # window mask combines decay mask and window_length(naive window mask)\n",
    "    window_length = args.window_length\n",
    "    _, _, current_length, _ = past[0][0].shape\n",
    "    #mask-part1-decay mask\n",
    "    if args.decay:\n",
    "        decay_mask = jt.arange(0., 1.0 + SmallConst, 1.0/(window_length))[1:]\n",
    "    else:\n",
    "        decay_mask = 1.0\n",
    "    #mask-part2-moving-window\n",
    "    if current_length > window_length and window_length > 0:\n",
    "        ones_key_val_shape = tuple(past[0][0].shape[:-2]) + tuple([window_length]) + tuple(\n",
    "            past[0][0].shape[-1:]) #shape: (batch_size, num_heads) + (seq_length) + (n_embd)\n",
    "\n",
    "        zeros_key_val_shape = tuple(past[0][0].shape[:-2]) + tuple([current_length - window_length]) + tuple(\n",
    "            past[0][0].shape[-1:])\n",
    "\n",
    "        ones_mask = jt.ones(ones_key_val_shape)\n",
    "        ones_mask = decay_mask*ones_mask.permute(0, 1, 3, 2)\n",
    "        ones_mask = ones_mask.permute(0, 1, 3, 2)\n",
    "\n",
    "        window_mask = jt.concat((ones_mask, jt.zeros(zeros_key_val_shape)), dim=-2)\n",
    "    else:\n",
    "        window_mask = jt.ones_like(past[0][0])\n",
    "    #====================================perturb the past for args.num_iteration times, which is similar to do 'optimizer step' for several times=================\n",
    "    def key_values_add(x,y):\n",
    "            return list(map(add, x,y))\n",
    "    loss_per_iter = []\n",
    "    for i in range(args.num_iterations):\n",
    "        perturbed_past = list(map(key_values_add, past, accu_grad))\n",
    "        _, _, current_length, _ = accu_grad[0][0].shape\n",
    "        # Compute hidden using perturbed past\n",
    "        result = model(prev, past_key_values=perturbed_past)\n",
    "        hidden = result['hidden_states'][-1] #(batch_size, 1, 1024)\n",
    "        new_accumulated_hidden = accumulated_hidden + jt.sum(hidden, dim=1) #(batch_size, 1024)\n",
    "\n",
    "        logits = result['logits']\n",
    "        logits = logits[:, -1, :]\n",
    "        probabs = jt.nn.softmax(logits, dim=-1)\n",
    "        #========================================================================calculate the loss================================================================\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        #bag of words\n",
    "        if args.loss_type == 1 or args.loss_type == 3:\n",
    "            for one_hot_good in one_hot_vectors:\n",
    "                good_logits = jt.matmul(probabs, jt.transpose(one_hot_good))\n",
    "                loss_word = good_logits\n",
    "                loss_word = jt.sum(loss_word)\n",
    "                loss_word = -jt.log(loss_word)\n",
    "    \n",
    "                loss += loss_word\n",
    "                loss_list.append(loss_word)\n",
    "            print('words', loss)\n",
    "\n",
    "        if args.loss_type == 2 or args.loss_type == 3:\n",
    "            new_true_past = true_past\n",
    "            #after using prev to query past, then do args.horizon_length times query, to better calculate the discriminator loss\n",
    "            for i in range(args.horizon_length):\n",
    "                future_probabs = jt.unsqueeze(probabs, dim=1)\n",
    "                embeds = jt.matmul(future_probabs, model.transformer.wte.weight)\n",
    "                \n",
    "                res= model(inputs_embeds= embeds, past_key_values=new_true_past)\n",
    "                new_true_past = res['past_key_values']\n",
    "                future_hidden = res['hidden_states'][-1]  # Get expected hidden states\n",
    "                new_accumulated_hidden = new_accumulated_hidden + jt.sum(future_hidden, dim=1)\n",
    "            predicted_sentiment = classifier(new_accumulated_hidden / (current_length + 1 + args.horizon_length))\n",
    "            \n",
    "            label = jt.array([args.label_class for i in range(predicted_sentiment.shape[0])], dtype=jt.int64)\n",
    "            discrim_loss = jt.nn.cross_entropy_loss(predicted_sentiment, label)\n",
    "            if args.log_level > 0:\n",
    "                print('discrim', discrim_loss)\n",
    "            loss += discrim_loss\n",
    "            loss_list.append(discrim_loss)\n",
    "\n",
    "\n",
    "        kl_loss = 0.0\n",
    "        if kl_scale > 0.0:\n",
    "            p = (jt.nn.softmax(original_probs[:, -1:, :], dim=-1)).squeeze(dim=1) #(batch_size, 50256)\n",
    "           \n",
    "            p = p + SmallConst * (p <= SmallConst)\n",
    "            correction = SmallConst * (probabs <= SmallConst)\n",
    "            corrected_probabs = probabs + correction\n",
    "            kl_loss = kl_scale * ((corrected_probabs * jt.log(corrected_probabs / p)).sum())\n",
    "            loss += kl_loss\n",
    "        if args.log_level > 0:\n",
    "            print('Discrim Loss: ',(loss - kl_loss))\n",
    "        loss_per_iter.append(loss)\n",
    "        #========================================================================get the grad of past========================================================================\n",
    "        grad = []\n",
    "        for layer in perturbed_past:\n",
    "            grad.append([jt.grad(loss, block, retain_graph=False) for block in layer])\n",
    "        def my_norm(x):\n",
    "            return (x.sqr()).sum().sqrt()\n",
    "        if grad_norms is not None and args.loss_type == 1:\n",
    "            grad_norms = [jt.max(grad_norms[index], my_norm(p_.grad * window_mask)) for index, p_ in\n",
    "                          enumerate(accu_grad)]\n",
    "        else:\n",
    "            grad_norms = []\n",
    "            for _,layer in enumerate(grad):\n",
    "                grad_norms.append([(my_norm(block * window_mask) + SmallConst) for block in layer])\n",
    "\n",
    "        perturb_grad = []\n",
    "        for i, layer in enumerate(accu_grad):\n",
    "            perturb_grad.append([-stepsize * (grad[i][j] * window_mask / grad_norms[i][j] ** args.gamma) for j, _ in enumerate(layer)])\n",
    "        \n",
    "        accu_grad = list(map(key_values_add, perturb_grad, accu_grad))\n",
    "\n",
    "    perturbed_past = list(map(key_values_add, past, accu_grad))\n",
    "    return perturbed_past, new_accumulated_hidden, grad_norms, loss_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_hidden(model, args, classifier, context=None, past=None,\n",
    "                       sample=False, perturb=True, good_index=None):\n",
    "    \n",
    "    output = jt.int64(context).unsqueeze(0) if context else None\n",
    "    grad_norms = None\n",
    "    true_discrim_loss = []\n",
    "    loss_per_word = []\n",
    "    for i in trange(args.length, ascii=True):\n",
    "        # Get past/probs for current output, except for last word\n",
    "        # Note that GPT takes 2 inputs: past + current-token\n",
    "        if past is None and output is not None:\n",
    "            prev = output[:, -1:]\n",
    "            res =  model(output[:, :-1])\n",
    "            past = res['past_key_values']\n",
    "\n",
    "        res = model(output)\n",
    "        original_probs, true_past = res['logits'], res['past_key_values']\n",
    "        true_hidden = res['hidden_states'][-1]\n",
    "\n",
    "        if i >= args.grad_length:\n",
    "            current_stepsize = args.stepsize * 0\n",
    "        else:\n",
    "            current_stepsize = args.stepsize\n",
    "\n",
    "        #not perturb\n",
    "        if not perturb or args.num_iterations == 0:\n",
    "            perturbed_past = past\n",
    "        #perturb\n",
    "        else:\n",
    "            accumulated_hidden = true_hidden[:, :-1, :] #all hidden states except query word\n",
    "            accumulated_hidden = jt.sum(accumulated_hidden, dim=1)\n",
    "            perturbed_past, _, grad_norms, loss_per_iter = perturb_past(past, model, prev, args,\n",
    "                                                                        classifier=classifier,\n",
    "                                                                        good_index=good_index, stepsize=current_stepsize,\n",
    "                                                                        original_probs=original_probs,\n",
    "                                                                        true_past=true_past,\n",
    "                                                                        accumulated_hidden=accumulated_hidden,\n",
    "                                                                        grad_norms=grad_norms)\n",
    "            loss_per_word.append(loss_per_iter)\n",
    "\n",
    "        if classifier is not None:\n",
    "            ce_loss = jt.nn.CrossEntropyLoss()\n",
    "            predicted_sentiment = classifier(jt.mean(true_hidden, dim=1))\n",
    "            label = jt.int64([args.label_class])\n",
    "            true_discrim_loss.append(ce_loss(predicted_sentiment, label))\n",
    "            if args.log_level > 0:\n",
    "                print(\"true discrim loss\", true_discrim_loss[-1])\n",
    "        else:\n",
    "            true_discrim_loss.append(0.0)\n",
    "        \n",
    "        res = model(prev, past_key_values=perturbed_past)\n",
    "        test_logits, past = res['logits'], res['past_key_values']\n",
    "        logits = test_logits\n",
    "        logits = logits[:, -1, :] / args.temperature \n",
    "        log_probs = jt.nn.softmax(logits, dim=-1)\n",
    "\n",
    "        # Fuse the modified model and original model\n",
    "        if perturb:\n",
    "            original_probs = jt.nn.softmax(original_probs[:, -1, :], dim=-1)\n",
    "            gm_scale = args.fusion_gm_scale\n",
    "            log_probs = ((log_probs ** gm_scale) * (original_probs ** (1 - gm_scale)))  \n",
    "            log_probs = top_k_logits(log_probs, k=args.top_k, probs=True) \n",
    "\n",
    "            if jt.sum(log_probs) <= 1:\n",
    "                log_probs = log_probs / jt.sum(log_probs)\n",
    "        else:\n",
    "            pert_logits = top_k_logits(logits, k=args.top_k)  \n",
    "            log_probs = jt.nn.softmax(pert_logits, dim=-1)\n",
    "            \n",
    "        if sample:\n",
    "            prev, best_ll = None, None\n",
    "            for idx in range(args.sampling_nums):\n",
    "                temp_prev = jt.multinomial(log_probs, num_samples=1)\n",
    "\n",
    "                temp = temp_prev.clone() if output is None else jt.concat((output, temp_prev), dim=1)\n",
    "                \n",
    "                result = model(temp)\n",
    "                temp_hidden = result.last_hidden_state[0]\n",
    "                ll = classifier(temp_hidden.mean(dim=0))[args.label_lass]\n",
    "                dist_score = (dist_n(temp,1) + dist_n(temp, 2) + dist_n(temp, 3)) / 3\n",
    "                \n",
    "                if (best_ll is None) or (ll > best_ll and dist_score > args.dist_threshold):\n",
    "                    prev = temp\n",
    "                    best_ll = ll\n",
    "        else:\n",
    "            prev = jt.multinomial(log_probs, num_samples=1)\n",
    "            \n",
    "        output = prev if output is None else jt.concat((output, prev), dim=1)  # update output\n",
    "        if args.log_level > 0:\n",
    "            print(enc.decode(output.tolist()[0]))\n",
    "\n",
    "    return output, true_discrim_loss, loss_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def latent_perturb(model, args, context=None, sample=True):\n",
    "    if args.discrim == 'sentiment':\n",
    "        classifier = ClassificationHead(class_size=5, embed_size=1024)\n",
    "        classifier.load(\"discrim_models/sentiment_classifierhead.pt\")\n",
    "        classifier.eval()\n",
    "        if args.label_class < 0:\n",
    "            raise Exception('Wrong class for sentiment, use --label-class 2 for *very positive*, 3 for *very negative*')\n",
    "    else:\n",
    "        classifier = None\n",
    "    # Get tokens for the list of positive words\n",
    "    def list_tokens(word_list):\n",
    "        token_list = []\n",
    "        for word in word_list:\n",
    "            token_list.append(enc.encode(\" \" + word))\n",
    "        return token_list\n",
    "\n",
    "    good_index = []\n",
    "    if args.bag_of_words:\n",
    "        bags_of_words = args.bag_of_words.split(\";\")\n",
    "        for wordlist in bags_of_words:\n",
    "            with open(wordlist, \"r\") as f:\n",
    "                words = f.read()\n",
    "                words = words.split('\\n')\n",
    "            good_index.append(list_tokens(words)) # good_index is the encode of the words\n",
    "        \n",
    "    if args.bag_of_words and classifier:\n",
    "        if args.log_level > 0:\n",
    "            print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n",
    "        args.loss_type = 3\n",
    "\n",
    "    elif args.bag_of_words:\n",
    "        args.loss_type = 1\n",
    "        if args.log_level > 0:\n",
    "            print('Using PPLM-BoW')\n",
    "\n",
    "    elif classifier is not None:\n",
    "        args.loss_type = 2\n",
    "        if args.log_level > 0:\n",
    "            print('Using PPLM-Discrim')\n",
    "    else:\n",
    "        raise Exception('Supply either --bag-of-words (-B) or --discrim -D')\n",
    "    #==================================================generate non perturbed words=======================================================\n",
    "    original, _, _ = sample_from_hidden(model=model, args=args, context=context,\n",
    "                                  perturb=False, good_index=good_index, classifier=classifier)\n",
    "\n",
    "    #==================================================generate perturbed words===========================================================\n",
    "    perturbed_list = []\n",
    "    discrim_loss_list = []\n",
    "    loss_per_sen = []\n",
    "\n",
    "    for i in range(args.num_samples): #another implementation of resample (with obvious improvement), but much higher computation cost, so usually set num_samples=1\n",
    "        perturbed, discrim_loss, loss_per_word = sample_from_hidden(model=model, args=args, context=context,\n",
    "                                                        perturb=True, good_index=good_index,\n",
    "                                                         classifier=classifier)\n",
    "        perturbed_list.append(perturbed)\n",
    "        if classifier is not None:\n",
    "            discrim_loss_list.append(discrim_loss)\n",
    "        loss_per_sen.append(loss_per_word)\n",
    "    \n",
    "    return original, perturbed_list, discrim_loss_list, loss_per_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = gpt2.GPT2Config()\n",
    "model = gpt2.GPT2LMHeadModel(config)\n",
    "model.load('gpt2-medium.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#choose -B or -D\n",
    "parser.add_argument('--bag-of-words', '-B', type=str, default=None, \n",
    "                    help='Bags of words used for PPLM-BoW. Multiple BoWs separated by ;')\n",
    "parser.add_argument('--discrim', '-D', type=str, default='sentiment')\n",
    "parser.add_argument('--label-class', type=int, default=2, help='Class label used for the discriminator')\n",
    "#BR\n",
    "parser.add_argument('--stepsize', type=float, default=0.03, help='the lr for updating past-key-values')\n",
    "parser.add_argument('--num-iterations', type=int, default=0, help='iterations for updating past-key-values')\n",
    "parser.add_argument('--horizon-length', type=int, default=1, help='Length of future to optimize over')\n",
    "parser.add_argument('--grad-length', type=int, default=10000, help='the upper bound for times of updating gradients')\n",
    "parser.add_argument('--window-length', type=int, default=0,\n",
    "                    help='Length of past which is being optimizer; 0 corresponds to infinite window length')\n",
    "parser.add_argument('--decay', action='store_true', help='whether to decay or not')\n",
    "parser.add_argument('--gamma', type=float, default=1.0)\n",
    "#BC\n",
    "parser.add_argument('--sampling_nums', type=int, default=1)\n",
    "parser.add_argument('--dist_threshold', type=float, default=0.8)#0.9\n",
    "#general settings\n",
    "parser.add_argument(\"--length\", type=int, default=15, help='length of the generated sentences(exclude the prefix)')\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "parser.add_argument(\"--top_k\", type=int, default=10)\n",
    "parser.add_argument(\"--fusion-gm-scale\", type=float, default=0.95)\n",
    "parser.add_argument(\"--fusion-kl-scale\", type=float, default=0.01)\n",
    "parser.add_argument('--log_level',type=int, default=0,help='0 for no log, 1 for log')\n",
    "parser.add_argument('--num-samples', type=int, default=1,\n",
    "                    help='Number of samples to generate from the modified latents')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# 设置随机种子\n",
    "jt.core.set_seed(args.seed)\n",
    "\n",
    "# use cuda\n",
    "jt.flags.use_cuda = jt.has_cuda\n",
    "\n",
    "prefix = ['Once upon a time', 'The book', 'The chicken', 'The city'\n",
    "          ,'The country', 'The horse', 'The lake', 'The last time'\n",
    "          ,'The movie', 'The painting', 'The pizza', 'The potato',\n",
    "          'The president of the country', 'The road', 'The year is 1910.']\n",
    "\n",
    "seq = [[50256] + enc.encode(p) for p in prefix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current index 0, current / all = 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Compiling Operators(25/63) used: 2.35s eta: 3.57s 27/63) used: 3.35s eta: 4.47s 29/63) used: 4.36s eta: 5.11s 31/63) used: 5.37s eta: 5.54s 34/63) used: 6.38s eta: 5.44s 36/63) used: 7.39s eta: 5.54s 39/63) used:  8.4s eta: 5.17s 41/63) used:  9.4s eta: 5.05s 43/63) used: 10.4s eta: 4.84s 45/63) used: 11.4s eta: 4.57s 47/63) used: 11.4s eta: 3.89s 49/63) used: 12.4s eta: 3.55s 51/63) used: 13.4s eta: 3.16s 54/63) used: 14.4s eta: 2.41s 56/63) used: 15.4s eta: 1.93s 60/63) used: 16.5s eta: 0.823s 63/63) used: 17.5s eta:    0s \n"
     ]
    }
   ],
   "source": [
    "collect_gen = dict() #record the output\n",
    "current_index = 0 \n",
    "for idx, out in enumerate(seq):\n",
    "    print('Current index {}, current / all = {}%'.format(idx, idx / len(seq) * 100))\n",
    "    if args.log_level > 0:\n",
    "        text = enc.decode(out)\n",
    "        print(\"=\" * 40 + \" Prefix of sentence \" + \"=\" * 40)\n",
    "        print(text)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    origin_tokens, perturb_tokens, discrim_loss_list, loss_in_time_list = latent_perturb(model=model, args=args, context=out)\n",
    "\n",
    "    origin_output = enc.decode(origin_tokens.tolist()[0])\n",
    "    if args.log_level > 0:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 40 + \" Whole sentence (Original)\" + \"=\" * 40)\n",
    "        print(origin_output)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    if args.num_samples > 1:\n",
    "        sen_discrim_loss = []\n",
    "        for sen in loss_in_time_list:\n",
    "            sen_loss = [jt.concat(x).mean() for x in sen]\n",
    "            sen_discrim_loss.append(jt.concat(sen_loss).mean())\n",
    "        sen_discrim_loss = jt.concat(sen_discrim_loss)\n",
    "        index ,_ = sen_discrim_loss.argmax(dim=0)\n",
    "        index = int(index.data)\n",
    "        perturb_tokens = [perturb_tokens[index]]\n",
    "\n",
    "    # generated = 0\n",
    "    # 干扰后的结果\n",
    "    for perturb_sen in perturb_tokens:\n",
    "        if args.log_level > 0:\n",
    "            print(\"=\" * 40 + \" Whole sentence (Perturbed)\" + \"=\" * 40)\n",
    "            perturb_output = enc.decode(perturb_sen.tolist()[0])\n",
    "            print(perturb_output)\n",
    "            print(\"=\" * 80)\n",
    "        collect_gen[current_index] = [out, perturb_tokens, origin_tokens]\n",
    "        current_index = current_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = []\n",
    "perturb = []\n",
    "for idx in range(current_index):\n",
    "    sen = collect_gen[idx]\n",
    "    origin.append(sen[2].tolist()[0])\n",
    "    perturb.append(sen[1][0].tolist()[0])\n",
    "#dist-n\n",
    "dist_score1 = dist_n(origin, 1)\n",
    "dist_score2 = dist_n(origin, 2)\n",
    "dist_score3 = dist_n(origin, 3)\n",
    "\n",
    "dist_score1p = dist_n(perturb, 1)\n",
    "dist_score2p = dist_n(perturb, 2)\n",
    "dist_score3p = dist_n(perturb, 3)\n",
    "\n",
    "print('origin: dist-1 {}, dist-2 {}, dist-3 {}\\n'.format(dist_score1, dist_score2, dist_score3))\n",
    "print('perturbed: dist-1 {}, dist-2 {}, dist-3 {}\\n'.format(dist_score1p, dist_score2p, dist_score3p))\n",
    "\n",
    "#sentiment accuracy\n",
    "sentiment_pipeline = transformers.pipeline(\"sentiment-analysis\")\n",
    "data= []\n",
    "for sen in origin:\n",
    "    data.append(enc.decode(sen)[1:])\n",
    "predict = sentiment_pipeline(data)\n",
    "count = 0\n",
    "length = len(origin)\n",
    "for idx in range(length):\n",
    "    if predict[idx]['label'] == 'POSITIVE':\n",
    "        count += 1\n",
    "origin_posi = count/length\n",
    "print('origin positive rate = ',origin_posi)\n",
    "print('origin negative rate = ',1 - origin_posi)\n",
    "data= []\n",
    "for sen in perturb:\n",
    "    data.append(enc.decode(sen)[1:])\n",
    "predict = sentiment_pipeline(data)\n",
    "count = 0\n",
    "length = len(perturb)\n",
    "for idx in range(length):\n",
    "    if predict[idx]['label'] == 'POSITIVE':\n",
    "        count += 1\n",
    "perturb_posi = count/length\n",
    "print('perturb positive rate = ',perturb_posi)\n",
    "print('perturb negative rate = ',1 - perturb_posi)\n",
    "\n",
    "#perplexity\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "origin_decoded = [enc.decode(sen) for sen in origin]\n",
    "perturb_decoded = [enc.decode(sen) for sen in perturb]\n",
    "origin_ppl = perplexity.compute(predictions=origin_decoded, model_id='gpt2')\n",
    "perturb_ppl = perplexity.compute(predictions=perturb_decoded, model_id='gpt2') \n",
    "print('origin mean perplexity: {}, perturbed mean perplexity: {}'.format(origin_ppl['mean_perplexity'],perturb_ppl['mean_perplexity']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "54f0cfbc081d2dbd9b32bca22e677f6cddd072e682c5c874abb322034cb62c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
