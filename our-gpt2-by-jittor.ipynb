{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### GPT MODEL"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import math\n","import os\n","from dataclasses import dataclass\n","from typing import Optional, Tuple, Union, List, Set, OrderedDict, Dict, Any\n","import warnings\n","import jittor as jt\n","from jittor import nn\n","import torch\n","import transformers\n","from transformers.modeling_outputs import (\n","    BaseModelOutputWithPastAndCrossAttentions\n",")\n","\n","class ClassInstantier(OrderedDict):\n","    def __getitem__(self, key):\n","        content = super().__getitem__(key)\n","        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n","        return cls(**kwargs)\n","\n","class NewGELUActivation(nn.Module):\n","    \"\"\"\n","    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n","    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    def __init__(self, *args, **kw) -> None:\n","        super().__init__(*args, **kw)\n","    def execute(self, input: jt.array) -> jt.array:\n","        return 0.5 * input * (1.0 + jt.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * jt.pow(input, 3.0))))\n","ACT2CLS = {\n","    # \"gelu\": GELUActivation,\n","    # \"gelu_10\": (ClippedGELUActivation, {\"min\": -10, \"max\": 10}),\n","    # \"gelu_fast\": FastGELUActivation,\n","    \"gelu_new\": NewGELUActivation,\n","    # \"gelu_python\": (GELUActivation, {\"use_gelu_python\": True}),\n","    # \"linear\": LinearActivation,\n","    # \"mish\": MishActivation,\n","    # \"quick_gelu\": QuickGELUActivation,\n","    \"relu\": nn.ReLU,\n","    \"relu6\": nn.ReLU6,\n","    \"sigmoid\": nn.Sigmoid,\n","    # \"silu\": SiLUActivation,\n","    # \"swish\": SiLUActivation,\n","    \"tanh\": nn.Tanh,\n","}\n","ACT2FN = ClassInstantier(ACT2CLS)\n","from transformers.modeling_outputs import (\n","    #BaseModelOutputWithPastAndCrossAttentions,\n","    CausalLMOutputWithCrossAttentions\n",")\n","\n","# from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n","from transformers.utils import (\n","    ModelOutput,\n","    add_code_sample_docstrings,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    logging,\n","    replace_return_docstrings,\n",")\n","from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n","# from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","\n","logger = logging.get_logger(__name__)\n","\n","_CHECKPOINT_FOR_DOC = \"gpt2\"\n","_CONFIG_FOR_DOC = \"GPT2Config\"\n","_TOKENIZER_FOR_DOC = \"GPT2Tokenizer\"\n","\n","GPT2_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"gpt2\",\n","    \"gpt2-medium\",\n","    \"gpt2-large\",\n","    \"gpt2-xl\",\n","    \"distilgpt2\",\n","    # See all GPT-2 models at https://huggingface.co/models?filter=gpt2\n","]\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class Conv1D(nn.Module):\n","    def __init__(self, nf, nx):\n","        super().__init__()\n","        self.nf = nf\n","        self.weight = jt.normal(0,0.02,size=(nx, nf))\n","        self.bias = jt.zeros(nf)\n","\n","    def execute(self, x):\n","        shape_out = x.shape[:-1] + (self.nf,)\n","        x = jt.matmul(x.reshape(-1, x.shape[-1]), self.weight) + self.bias\n","        x = x.reshape(shape_out)\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["##### hook"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["attn_grad = [ [] for i in range(24)]\n","current_layer = 0\n","def attn_hook(x):\n","    attn_grad[current_layer].append(x)\n","for block in attn_grad:\n","    block.append(jt.zeros(()))"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["class GPT2Attention(jt.nn.Module):\n","    def __init__(self, config, is_cross_attention=False, layer_idx=None) -> None:\n","        super().__init__()\n","\n","        max_positions = config.n_positions\n","        self.bias = jt.tril(jt.ones((max_positions, max_positions), dtype=jt.uint8)).reshape(\n","            (1,1,max_positions,max_positions)\n","            ) #lower trangle matrix\n","        self.masked_bias = jt.array(-1e4)\n","\n","        self.embed_dim = config.hidden_size\n","        self.num_heads = config.num_attention_heads\n","        self.head_dim = self.embed_dim // self.num_heads\n","        self.split_size = self.embed_dim\n","        if self.head_dim * self.num_heads != self.embed_dim:\n","            raise ValueError(\n","                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n","                f\" {self.num_heads}).\"\n","            )\n","\n","        self.scale_attn_weights = config.scale_attn_weights\n","        self.is_cross_attention = is_cross_attention\n","\n","        # Layer-wise attention scaling, reordering, and upcasting\n","        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n","        self.layer_idx = layer_idx\n","        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn\n","\n","        if self.is_cross_attention:\n","            self.c_attn = Conv1D(2 * self.embed_dim, self.embed_dim)\n","            self.q_attn = Conv1D(self.embed_dim, self.embed_dim)\n","        else:\n","            self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n","        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n","\n","        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","\n","        self.pruned_heads = set()\n","        \n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n","        index_attn = jt.concat([index, index + self.split_size, index + (2 * self.split_size)])\n","\n","        # Prune conv1d layers\n","        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n","        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n","\n","        # Update hyper params\n","        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n","        self.num_heads = self.num_heads - len(heads)\n","        self.pruned_heads = self.pruned_heads.union(heads)\n","\n","    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n","        attn_weights = jt.matmul(query, key.transpose(-1, -2))\n","        key.register_hook(lambda x:print(x))\n","\n","        if self.scale_attn_weights:\n","            attn_weights = attn_weights / jt.full(\n","                [], value.shape[-1] ** 0.5, dtype=attn_weights.dtype\n","            )\n","\n","        # Layer-wise attention scaling\n","        if self.scale_attn_by_inverse_layer_idx:\n","            attn_weights = attn_weights / float(self.layer_idx + 1)\n","\n","        if not self.is_cross_attention:\n","            # if only \"normal\" attention layer implements causal mask\n","            query_length, key_length = query.size(-2), key.size(-2)\n","            batch_size, num_heads = query.size(0), query.size(1)\n","            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()\n","            causal_mask = causal_mask.repeat(batch_size, num_heads, 1 , 1)\n","            mask_value = 1e-32#torch.finfo(attn_weights.dtype).min\n","            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n","            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n","            mask_value = jt.full([], mask_value, dtype=attn_weights.dtype)\n","            attn_weights = jt.where(causal_mask, attn_weights, mask_value)\n","\n","        if attention_mask is not None:\n","            # Apply the attention mask\n","            attn_weights = attn_weights + attention_mask\n","\n","        attn_weights = nn.softmax(attn_weights, dim=-1)\n","\n","        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n","        # attn_weights = attn_weights.type(value.dtype)\n","        attn_weights = self.attn_dropout(attn_weights)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attn_weights = attn_weights * head_mask\n","\n","        attn_output = jt.matmul(attn_weights, value)\n","        value.register_hook(attn_hook)\n","        return attn_output, attn_weights\n","\n","    def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n","        # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n","        bsz, num_heads, q_seq_len, dk = query.size()\n","        _, _, k_seq_len, _ = key.size()\n","\n","        # Preallocate attn_weights for `baddbmm`\n","        attn_weights = jt.empty(bsz * num_heads, q_seq_len, k_seq_len, dtype=jt.float32)\n","\n","        # Compute Scale Factor\n","        scale_factor = 1.0\n","        if self.scale_attn_weights:\n","            scale_factor /= float(value.size(-1)) ** 0.5\n","\n","        if self.scale_attn_by_inverse_layer_idx:\n","            scale_factor /= float(self.layer_idx + 1)\n","\n","        # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n","        # with autocast(enabled=False):\n","        q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n","        attn_weights = scale_factor * jt.bmm(q.float(),k.float())\n","        # attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n","        attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)\n","\n","        if not self.is_cross_attention:\n","            # if only \"normal\" attention layer implements causal mask\n","            query_length, key_length = query.size(-2), key.size(-2)\n","            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()\n","            mask_value = 1e-32 #torch.finfo(attn_weights.dtype).min\n","            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n","            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n","            mask_value = jt.array(mask_value, dtype=attn_weights.dtype)\n","            attn_weights = jt.where(causal_mask, attn_weights, mask_value)\n","\n","        if attention_mask is not None:\n","            # Apply the attention mask\n","            attn_weights = attn_weights + attention_mask\n","\n","        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n","\n","        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\n","        if attn_weights.dtype != jt.float32:\n","            raise RuntimeError(\"Error with upcasting, attn_weights does not have dtype jt.float32\")\n","        attn_weights = attn_weights.type(value.dtype)\n","        attn_weights = self.attn_dropout(attn_weights)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attn_weights = attn_weights * head_mask\n","\n","        attn_output = jt.matmul(attn_weights, value)\n","\n","        return attn_output, attn_weights\n","    \n","    def _split_heads(self, tensor, num_heads, attn_head_size):\n","        \"\"\"\n","        Splits hidden_size dim into attn_head_size and num_heads\n","        \"\"\"\n","        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n","        tensor = tensor.view(new_shape)\n","        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n","\n","    def _merge_heads(self, tensor, num_heads, attn_head_size):\n","        \"\"\"\n","        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n","        \"\"\"\n","        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n","        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n","        return tensor.view(new_shape)\n","\n","    def execute(\n","        self,\n","        hidden_states: Optional[Tuple[jt.array]], #[batch_size, seq_len, embedding_size]\n","        layer_past: Optional[Tuple[jt.array]] = None,\n","        attention_mask: Optional[jt.array] = None,\n","        head_mask: Optional[jt.array] = None,\n","        encoder_hidden_states: Optional[jt.array] = None,\n","        encoder_attention_mask: Optional[jt.array] = None,\n","        use_cache: Optional[bool] = False,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[Union[jt.array, Tuple[jt.array]], ...]:\n","        if encoder_hidden_states is not None:\n","            if not hasattr(self, \"q_attn\"):\n","                raise ValueError(\n","                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n","                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n","                )\n","\n","            query = self.q_attn(hidden_states)\n","            print(self.c_attn(encoder_hidden_states).shape)\n","            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n","            attention_mask = encoder_attention_mask\n","        else:\n","            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n","\n","        query = self._split_heads(query, self.num_heads, self.head_dim)\n","        key = self._split_heads(key, self.num_heads, self.head_dim)\n","        value = self._split_heads(value, self.num_heads, self.head_dim)\n","        \n","        if layer_past is not None:\n","            past_key, past_value = layer_past\n","            key = jt.concat((past_key, key), dim=-2)\n","            value = jt.concat((past_value, value), dim=-2)\n","\n","        if use_cache is True:\n","            present = (key, value)\n","        else:\n","            present = None\n","\n","        if self.reorder_and_upcast_attn:\n","            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n","        else:\n","            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n","\n","        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n","        attn_output = self.c_proj(attn_output)\n","        attn_output = self.resid_dropout(attn_output)\n","\n","        outputs = (attn_output, present)\n","        if output_attentions:\n","            outputs += (attn_weights,)\n","\n","        return outputs  # a, present, (attentions)\n","\n"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["class GPT2MLP(nn.Module):\n","    def __init__(self, intermediate_size, config):\n","        super().__init__()\n","        embed_dim = config.hidden_size\n","        self.c_fc = Conv1D(intermediate_size, embed_dim)\n","        self.c_proj = Conv1D(embed_dim, intermediate_size)\n","        self.act = ACT2FN[config.activation_function]\n","        self.dropout = nn.Dropout(config.resid_pdrop)\n","\n","    def execute(self, hidden_states: Optional[Tuple[jt.array]]) -> jt.array:\n","        hidden_states = self.c_fc(hidden_states)\n","        hidden_states = self.act(hidden_states)\n","        hidden_states = self.c_proj(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        return hidden_states\n","\n"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["class GPT2Block(nn.Module):\n","    def __init__(self, config, layer_idx=None):\n","        super().__init__()\n","        hidden_size = config.hidden_size\n","        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size  #intermediate\n","\n","        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n","        self.attn = GPT2Attention(config, layer_idx=layer_idx)\n","        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n","\n","        if config.add_cross_attention:\n","            self.crossattention = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)\n","            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n","\n","        self.mlp = GPT2MLP(inner_dim, config)\n","    \n","    def execute(\n","        self,\n","        hidden_states: Optional[Tuple[jt.array]],\n","        layer_past: Optional[Tuple[jt.array]] = None,\n","        attention_mask: Optional[jt.array] = None,\n","        head_mask: Optional[jt.array] = None,\n","        encoder_hidden_states: Optional[jt.array] = None,\n","        encoder_attention_mask: Optional[jt.array] = None,\n","        use_cache: Optional[bool] = False,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Union[Tuple[jt.array], Optional[Tuple[jt.array, Tuple[jt.array, ...]]]]:\n","        residual = hidden_states\n","        hidden_states = self.ln_1(hidden_states)\n","        attn_outputs = self.attn(\n","            hidden_states,\n","            layer_past=layer_past,\n","            attention_mask=attention_mask,\n","            head_mask=head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","        )\n","        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n","        outputs = attn_outputs[1:]\n","        # residual connection\n","        hidden_states = attn_output + residual\n","\n","        if encoder_hidden_states is not None:\n","            # add one self-attention block for cross-attention\n","            if not hasattr(self, \"crossattention\"):\n","                raise ValueError(\n","                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n","                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n","                )\n","            residual = hidden_states\n","            hidden_states = self.ln_cross_attn(hidden_states)\n","            cross_attn_outputs = self.crossattention(\n","                hidden_states,\n","                attention_mask=attention_mask,\n","                head_mask=head_mask,\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_attention_mask,\n","                output_attentions=output_attentions,\n","            )\n","            attn_output = cross_attn_outputs[0]\n","            # residual connection\n","            hidden_states = residual + attn_output\n","            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n","\n","        residual = hidden_states\n","        hidden_states = self.ln_2(hidden_states)\n","        feed_forward_hidden_states = self.mlp(hidden_states)\n","        # residual connection\n","        hidden_states = residual + feed_forward_hidden_states\n","\n","        if use_cache:\n","            outputs = (hidden_states,) + outputs\n","        else:\n","            outputs = (hidden_states,) + outputs[1:]\n","\n","        return outputs  # hidden_states, present, (attentions, cross_attentions)\n","\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["class PretrainedConfig:\n","    r\"\"\"\n","    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n","    methods for loading/downloading/saving configurations.\n","\n","    <Tip>\n","\n","    A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to\n","    initialize a model does **not** load the model weights. It only affects the model's configuration.\n","\n","    </Tip>\n","\n","    Class attributes (overridden by derived classes):\n","\n","    - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate\n","      the correct object in [`~transformers.AutoConfig`].\n","    - **is_composition** (`bool`) -- Whether the config class is composed of multiple sub-configs. In this case the\n","      config has to be initialized from two or more configs of type [`~transformers.PretrainedConfig`] like:\n","      [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`].\n","    - **keys_to_ignore_at_inference** (`List[str]`) -- A list of keys to ignore by default when looking at dictionary\n","      outputs of the model during inference.\n","    - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n","      naming of attributes.\n","\n","    Common attributes (present in all subclasses):\n","\n","    - **vocab_size** (`int`) -- The number of tokens in the vocabulary, which is also the first dimension of the\n","      embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).\n","    - **hidden_size** (`int`) -- The hidden size of the model.\n","    - **num_attention_heads** (`int`) -- The number of attention heads used in the multi-head attention layers of the\n","      model.\n","    - **num_hidden_layers** (`int`) -- The number of blocks in the model.\n","\n","    Arg:\n","        name_or_path (`str`, *optional*, defaults to `\"\"`):\n","            Store the string that was passed to [`PreTrainedModel.from_pretrained`] or\n","            [`TFPreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path` if the configuration was created\n","            with such a method.\n","        output_hidden_states (`bool`, *optional*, defaults to `False`):\n","            Whether or not the model should return all hidden-states.\n","        output_attentions (`bool`, *optional*, defaults to `False`):\n","            Whether or not the model should returns all attentions.\n","        return_dict (`bool`, *optional*, defaults to `True`):\n","            Whether or not the model should return a [`~transformers.utils.ModelOutput`] instead of a plain tuple.\n","        is_encoder_decoder (`bool`, *optional*, defaults to `False`):\n","            Whether the model is used as an encoder/decoder or not.\n","        is_decoder (`bool`, *optional*, defaults to `False`):\n","            Whether the model is used as decoder or not (in which case it's used as an encoder).\n","        cross_attention_hidden_size** (`bool`, *optional*):\n","            The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder\n","            setting and the cross-attention hidden dimension differs from `self.config.hidden_size`.\n","        add_cross_attention (`bool`, *optional*, defaults to `False`):\n","            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n","            that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models\n","            in `AUTO_MODELS_FOR_CAUSAL_LM`.\n","        tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n","            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n","            and decoder model to have the exact same parameter names.\n","        prune_heads (`Dict[int, List[int]]`, *optional*, defaults to `{}`):\n","            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n","            heads to prune in said layer.\n","\n","            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n","        chunk_size_feed_forward (`int`, *optional*, defaults to `0`):\n","            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that\n","            the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <\n","            sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed\n","            Forward Chunking work?](../glossary.html#feed-forward-chunking).\n","\n","        > Parameters for sequence generation\n","\n","        max_length (`int`, *optional*, defaults to 20):\n","            Maximum length that will be used by default in the `generate` method of the model.\n","        min_length (`int`, *optional*, defaults to 10):\n","            Minimum length that will be used by default in the `generate` method of the model.\n","        do_sample (`bool`, *optional*, defaults to `False`):\n","            Flag that will be used by default in the `generate` method of the model. Whether or not to use sampling ;\n","            use greedy decoding otherwise.\n","        early_stopping (`bool`, *optional*, defaults to `False`):\n","            Flag that will be used by default in the `generate` method of the model. Whether to stop the beam search\n","            when at least `num_beams` sentences are finished per batch or not.\n","        num_beams (`int`, *optional*, defaults to 1):\n","            Number of beams for beam search that will be used by default in the `generate` method of the model. 1 means\n","            no beam search.\n","        num_beam_groups (`int`, *optional*, defaults to 1):\n","            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams\n","            that will be used by default in the `generate` method of the model. 1 means no group beam search.\n","        diversity_penalty (`float`, *optional*, defaults to 0.0):\n","            Value to control diversity for group beam search. that will be used by default in the `generate` method of\n","            the model. 0 means no diversity penalty. The higher the penalty, the more diverse are the outputs.\n","        temperature (`float`, *optional*, defaults to 1):\n","            The value used to module the next token probabilities that will be used by default in the `generate` method\n","            of the model. Must be strictly positive.\n","        top_k (`int`, *optional*, defaults to 50):\n","            Number of highest probability vocabulary tokens to keep for top-k-filtering that will be used by default in\n","            the `generate` method of the model.\n","        top_p (`float`, *optional*, defaults to 1):\n","            Value that will be used by default in the `generate` method of the model for `top_p`. If set to float < 1,\n","            only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.\n","        repetition_penalty (`float`, *optional*, defaults to 1):\n","            Parameter for repetition penalty that will be used by default in the `generate` method of the model. 1.0\n","            means no penalty.\n","        length_penalty (`float`, *optional*, defaults to 1):\n","            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n","            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n","            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n","            `length_penalty` < 0.0 encourages shorter sequences.\n","        no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by default in the\n","            `generate` method of the model for `no_repeat_ngram_size`. If set to int > 0, all ngrams of that size can\n","            only occur once.\n","        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by\n","            default in the `generate` method of the model for `encoder_no_repeat_ngram_size`. If set to int > 0, all\n","            ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`.\n","        bad_words_ids (`List[int]`, *optional*):\n","            List of token ids that are not allowed to be generated that will be used by default in the `generate`\n","            method of the model. In order to get the tokens of the words that should not appear in the generated text,\n","            use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n","        num_return_sequences (`int`, *optional*, defaults to 1):\n","            Number of independently computed returned sequences for each element in the batch that will be used by\n","            default in the `generate` method of the model.\n","        output_scores (`bool`, *optional*, defaults to `False`):\n","            Whether the model should return the logits when used for generation.\n","        return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n","            Whether the model should return a [`~transformers.utils.ModelOutput`] instead of a `jt.array`.\n","        forced_bos_token_id (`int`, *optional*):\n","            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for\n","            multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target\n","            language token.\n","        forced_eos_token_id (`int`, *optional*):\n","            The id of the token to force as the last generated token when `max_length` is reached.\n","        remove_invalid_values (`bool`, *optional*):\n","            Whether to remove possible _nan_ and _inf_ outputs of the model to prevent the generation method to crash.\n","            Note that using `remove_invalid_values` can slow down generation.\n","\n","        > Parameters for fine-tuning tasks\n","\n","        architectures (`List[str]`, *optional*):\n","            Model architectures that can be used with the model pretrained weights.\n","        finetuning_task (`str`, *optional*):\n","            Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow\n","            or PyTorch) checkpoint.\n","        id2label (`Dict[int, str]`, *optional*):\n","            A map from index (for instance prediction index, or target index) to label.\n","        label2id (`Dict[str, int]`, *optional*): A map from label to index for the model.\n","        num_labels (`int`, *optional*):\n","            Number of labels to use in the last layer added to the model, typically for a classification task.\n","        task_specific_params (`Dict[str, Any]`, *optional*):\n","            Additional keyword arguments to store for the current task.\n","        problem_type (`str`, *optional*):\n","            Problem type for `XxxForSequenceClassification` models. Can be one of `\"regression\"`,\n","            `\"single_label_classification\"` or `\"multi_label_classification\"`.\n","\n","        > Parameters linked to the tokenizer\n","\n","        tokenizer_class (`str`, *optional*):\n","            The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the\n","            model by default).\n","        prefix (`str`, *optional*):\n","            A specific prompt that should be added at the beginning of each text before calling the model.\n","        bos_token_id (`int`, *optional*): The id of the _beginning-of-stream_ token.\n","        pad_token_id (`int`, *optional*): The id of the _padding_ token.\n","        eos_token_id (`int`, *optional*): The id of the _end-of-stream_ token.\n","        decoder_start_token_id (`int`, *optional*):\n","            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.\n","        sep_token_id (`int`, *optional*): The id of the _separation_ token.\n","\n","        > PyTorch specific parameters\n","\n","        torchscript (`bool`, *optional*, defaults to `False`):\n","            Whether or not the model should be used with Torchscript.\n","        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n","            Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n","            model has a output word embedding layer.\n","        torch_dtype (`str`, *optional*):\n","            The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`\n","            (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved\n","            model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load\n","            `float16` weights. Since the config object is stored in plain text, this attribute contains just the\n","            floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the\n","            `\"float16\"` string.\n","\n","            This attribute is currently not being used during model loading time, but this may change in the future\n","            versions. But we can already start preparing for the future by saving the dtype with save_pretrained.\n","\n","        > TensorFlow specific parameters\n","\n","        use_bfloat16 (`bool`, *optional*, defaults to `False`):\n","            Whether or not the model should use BFloat16 scalars (only used by some TensorFlow models).\n","        tf_legacy_loss (`bool`, *optional*, defaults to `False`):\n","            Whether the model should use legacy TensorFlow losses. Legacy losses have variable output shapes and may\n","            not be XLA-compatible. This option is here for backward compatibility and will be removed in Transformers\n","            v5.\n","    \"\"\"\n","    model_type: str = \"\"\n","    is_composition: bool = False\n","    attribute_map: Dict[str, str] = {}\n","    _auto_class: Optional[str] = None\n","\n","    def __setattr__(self, key, value):\n","        if key in super().__getattribute__(\"attribute_map\"):\n","            key = super().__getattribute__(\"attribute_map\")[key]\n","        super().__setattr__(key, value)\n","\n","    def __getattribute__(self, key):\n","        if key != \"attribute_map\" and key in super().__getattribute__(\"attribute_map\"):\n","            key = super().__getattribute__(\"attribute_map\")[key]\n","        return super().__getattribute__(key)\n","\n","    def __init__(self, **kwargs):\n","        # Attributes with defaults\n","        self.return_dict = kwargs.pop(\"return_dict\", True)\n","        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n","        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n","        self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n","        self.torch_dtype = kwargs.pop(\"torch_dtype\", None)  # Only used by PyTorch models\n","        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n","        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)  # Only used by TensorFlow models\n","        self.pruned_heads = kwargs.pop(\"pruned_heads\", {})\n","        self.tie_word_embeddings = kwargs.pop(\n","            \"tie_word_embeddings\", True\n","        )  # Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.\n","\n","        # Is decoder is used in encoder-decoder models to differentiate encoder from decoder\n","        self.is_encoder_decoder = kwargs.pop(\"is_encoder_decoder\", False)\n","        self.is_decoder = kwargs.pop(\"is_decoder\", False)\n","        self.cross_attention_hidden_size = kwargs.pop(\"cross_attention_hidden_size\", None)\n","        self.add_cross_attention = kwargs.pop(\"add_cross_attention\", False)\n","        self.tie_encoder_decoder = kwargs.pop(\"tie_encoder_decoder\", False)\n","\n","        # Parameters for sequence generation\n","        self.max_length = kwargs.pop(\"max_length\", 20)\n","        self.min_length = kwargs.pop(\"min_length\", 0)\n","        self.do_sample = kwargs.pop(\"do_sample\", False)\n","        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n","        self.num_beams = kwargs.pop(\"num_beams\", 1)\n","        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n","        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n","        self.temperature = kwargs.pop(\"temperature\", 1.0)\n","        self.top_k = kwargs.pop(\"top_k\", 50)\n","        self.top_p = kwargs.pop(\"top_p\", 1.0)\n","        self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n","        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n","        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n","        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n","        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n","        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n","        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n","        self.chunk_size_feed_forward = kwargs.pop(\"chunk_size_feed_forward\", 0)\n","        self.output_scores = kwargs.pop(\"output_scores\", False)\n","        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n","        self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n","        self.forced_eos_token_id = kwargs.pop(\"forced_eos_token_id\", None)\n","        self.remove_invalid_values = kwargs.pop(\"remove_invalid_values\", False)\n","        self.exponential_decay_length_penalty = kwargs.pop(\"exponential_decay_length_penalty\", None)\n","        self.suppress_tokens = kwargs.pop(\"suppress_tokens\", None)\n","        self.begin_suppress_tokens = kwargs.pop(\"begin_suppress_tokens\", None)\n","\n","        # Fine-tuning task arguments\n","        self.architectures = kwargs.pop(\"architectures\", None)\n","        self.finetuning_task = kwargs.pop(\"finetuning_task\", None)\n","        self.id2label = kwargs.pop(\"id2label\", None)\n","        self.label2id = kwargs.pop(\"label2id\", None)\n","        if self.id2label is not None:\n","            num_labels = kwargs.pop(\"num_labels\", None)\n","            if num_labels is not None and len(self.id2label) != num_labels:\n","                logger.warning(\n","                    f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n","                    f\"{self.id2label}. The number of labels wil be overwritten to {self.num_labels}.\"\n","                )\n","            self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n","            # Keys are always strings in JSON so convert ids to int here.\n","        else:\n","            self.num_labels = kwargs.pop(\"num_labels\", 2)\n","\n","        # if self.torch_dtype is not None and isinstance(self.torch_dtype, str):\n","        #     # we will start using self.torch_dtype in v5, but to be consistent with\n","        #     # from_pretrained's torch_dtype arg convert it to an actual torch.dtype object\n","        #     if is_torch_available():\n","        #         import torch\n","\n","        #         self.torch_dtype = getattr(torch, self.torch_dtype)\n","\n","        # Tokenizer arguments TODO: eventually tokenizer and models should share the same config\n","        self.tokenizer_class = kwargs.pop(\"tokenizer_class\", None)\n","        self.prefix = kwargs.pop(\"prefix\", None)\n","        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n","        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n","        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n","        self.sep_token_id = kwargs.pop(\"sep_token_id\", None)\n","\n","        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n","\n","        # task specific arguments\n","        self.task_specific_params = kwargs.pop(\"task_specific_params\", None)\n","\n","        # regression / multi-label classification\n","        self.problem_type = kwargs.pop(\"problem_type\", None)\n","        allowed_problem_types = (\"regression\", \"single_label_classification\", \"multi_label_classification\")\n","        if self.problem_type is not None and self.problem_type not in allowed_problem_types:\n","            raise ValueError(\n","                f\"The config parameter `problem_type` was not understood: received {self.problem_type} \"\n","                \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n","            )\n","\n","        # TPU arguments\n","        if kwargs.pop(\"xla_device\", None) is not None:\n","            logger.warning(\n","                \"The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can \"\n","                \"safely remove it from your `config.json` file.\"\n","            )\n","\n","        # Name or path to the pretrained checkpoint\n","        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n","        # Config hash\n","        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n","\n","        # Drop the transformers version info\n","        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n","\n","        # Deal with gradient checkpointing\n","        if kwargs.get(\"gradient_checkpointing\", False):\n","            warnings.warn(\n","                \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n","                \"Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the \"\n","                \"`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\"\n","            )\n","\n","        # Additional attributes without default values\n","        for key, value in kwargs.items():\n","            try:\n","                setattr(self, key, value)\n","            except AttributeError as err:\n","                logger.error(f\"Can't set {key} with value {value} for {self}\")\n","                raise err\n","\n","    @property\n","    def name_or_path(self) -> str:\n","        return getattr(self, \"_name_or_path\", None)\n","\n","    @name_or_path.setter\n","    def name_or_path(self, value):\n","        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n","\n","    @property\n","    def use_return_dict(self) -> bool:\n","        \"\"\"\n","        `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.\n","        \"\"\"\n","        # If torchscript is set, force `return_dict=False` to avoid jit errors\n","        return self.return_dict and not self.torchscript\n","\n","    @property\n","    def num_labels(self) -> int:\n","        \"\"\"\n","        `int`: The number of labels for classification models.\n","        \"\"\"\n","        return len(self.id2label)\n","\n","    @num_labels.setter\n","    def num_labels(self, num_labels: int):\n","        if not hasattr(self, \"id2label\") or self.id2label is None or len(self.id2label) != num_labels:\n","            self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n","            self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n","\n","    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n","        \"\"\"\n","        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n","        [`~PretrainedConfig.from_pretrained`] class method.\n","\n","        Args:\n","            save_directory (`str` or `os.PathLike`):\n","                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n","            push_to_hub (`bool`, *optional*, defaults to `False`):\n","                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n","                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n","                namespace).\n","            kwargs:\n","                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n","        \"\"\"\n","        if os.path.isfile(save_directory):\n","            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n","\n","        os.makedirs(save_directory, exist_ok=True)\n","\n","        if push_to_hub:\n","            commit_message = kwargs.pop(\"commit_message\", None)\n","            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n","            repo_id, token = self._create_repo(repo_id, **kwargs)\n","            files_timestamps = self._get_files_timestamps(save_directory)\n","\n","        # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n","        # loaded from the Hub.\n","        # if self._auto_class is not None:\n","        #     custom_object_save(self, save_directory, config=self)\n","\n","        # # If we save using the predefined names, we can load using `from_pretrained`\n","        # output_config_file = os.path.join(save_directory, CONFIG_NAME)\n","\n","        # self.to_json_file(output_config_file, use_diff=True)\n","        # logger.info(f\"Configuration saved in {output_config_file}\")\n","\n","        if push_to_hub:\n","            self._upload_modified_files(\n","                save_directory, repo_id, files_timestamps, commit_message=commit_message, token=token\n","            )\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n","        r\"\"\"\n","        Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n","\n","        Args:\n","            pretrained_model_name_or_path (`str` or `os.PathLike`):\n","                This can be either:\n","\n","                - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n","                  huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or\n","                  namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n","                - a path to a *directory* containing a configuration file saved using the\n","                  [`~PretrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n","                - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.\n","            cache_dir (`str` or `os.PathLike`, *optional*):\n","                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n","                standard cache should not be used.\n","            force_download (`bool`, *optional*, defaults to `False`):\n","                Whether or not to force to (re-)download the configuration files and override the cached versions if\n","                they exist.\n","            resume_download (`bool`, *optional*, defaults to `False`):\n","                Whether or not to delete incompletely received file. Attempts to resume the download if such a file\n","                exists.\n","            proxies (`Dict[str, str]`, *optional*):\n","                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n","                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n","            use_auth_token (`str` or `bool`, *optional*):\n","                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n","                the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n","            revision (`str`, *optional*, defaults to `\"main\"`):\n","                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n","                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n","                identifier allowed by git.\n","\n","                <Tip>\n","\n","                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n","\n","                </Tip>\n","\n","            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n","                If `False`, then this function returns just the final configuration object.\n","\n","                If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\n","                dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\n","                part of `kwargs` which has not been used to update `config` and is otherwise ignored.\n","            subfolder (`str`, *optional*, defaults to `\"\"`):\n","                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n","                specify the folder name here.\n","            kwargs (`Dict[str, Any]`, *optional*):\n","                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n","                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n","                by the `return_unused_kwargs` keyword parameter.\n","\n","        Returns:\n","            [`PretrainedConfig`]: The configuration object instantiated from this pretrained model.\n","\n","        Examples:\n","\n","        ```python\n","        # We can't instantiate directly the base class *PretrainedConfig* so let's show the examples on a\n","        # derived class: BertConfig\n","        config = BertConfig.from_pretrained(\n","            \"bert-base-uncased\"\n","        )  # Download configuration from huggingface.co and cache.\n","        config = BertConfig.from_pretrained(\n","            \"./test/saved_model/\"\n","        )  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*\n","        config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\")\n","        config = BertConfig.from_pretrained(\"bert-base-uncased\", output_attentions=True, foo=False)\n","        assert config.output_attentions == True\n","        config, unused_kwargs = BertConfig.from_pretrained(\n","            \"bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\n","        )\n","        assert config.output_attentions == True\n","        assert unused_kwargs == {\"foo\": False}\n","        ```\"\"\"\n","        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n","        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n","            logger.warning(\n","                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n","                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n","            )\n","\n","        return cls.from_dict(config_dict, **kwargs)\n","\n"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["class GPT2Config(PretrainedConfig):\n","    \"\"\"\n","    This is the configuration class to store the configuration of a [`GPT2Model`] or a [`TFGPT2Model`]. It is used to\n","    instantiate a GPT-2 model according to the specified arguments, defining the model architecture. Instantiating a\n","    configuration with the defaults will yield a similar configuration to that of the GPT-2\n","    [gpt2](https://huggingface.co/gpt2) architecture.\n","\n","    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n","    documentation from [`PretrainedConfig`] for more information.\n","\n","\n","    Args:\n","        vocab_size (`int`, *optional*, defaults to 50257):\n","            Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the\n","            `inputs_ids` passed when calling [`GPT2Model`] or [`TFGPT2Model`].\n","        n_positions (`int`, *optional*, defaults to 1024):\n","            The maximum sequence length that this model might ever be used with. Typically set this to something large\n","            just in case (e.g., 512 or 1024 or 2048).\n","        n_embd (`int`, *optional*, defaults to 768):\n","            Dimensionality of the embeddings and hidden states.\n","        n_layer (`int`, *optional*, defaults to 12):\n","            Number of hidden layers in the Transformer encoder.\n","        n_head (`int`, *optional*, defaults to 12):\n","            Number of attention heads for each attention layer in the Transformer encoder.\n","        n_inner (`int`, *optional*, defaults to None):\n","            Dimensionality of the inner feed-forward layers. `None` will set it to 4 times n_embd\n","        activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n","            Activation function, to be selected in the list `[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]`.\n","        resid_pdrop (`float`, *optional*, defaults to 0.1):\n","            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n","        embd_pdrop (`int`, *optional*, defaults to 0.1):\n","            The dropout ratio for the embeddings.\n","        attn_pdrop (`float`, *optional*, defaults to 0.1):\n","            The dropout ratio for the attention.\n","        layer_norm_epsilon (`float`, *optional*, defaults to 1e-5):\n","            The epsilon to use in the layer normalization layers.\n","        initializer_range (`float`, *optional*, defaults to 0.02):\n","            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n","        summary_type (`string`, *optional*, defaults to `\"cls_index\"`):\n","            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n","            [`TFGPT2DoubleHeadsModel`].\n","\n","            Has to be one of the following options:\n","\n","                - `\"last\"`: Take the last token hidden state (like XLNet).\n","                - `\"first\"`: Take the first token hidden state (like BERT).\n","                - `\"mean\"`: Take the mean of all tokens hidden states.\n","                - `\"cls_index\"`: Supply a Tensor of classification token position (like GPT/GPT-2).\n","                - `\"attn\"`: Not implemented now, use multi-head attention.\n","        summary_use_proj (`bool`, *optional*, defaults to `True`):\n","            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n","            [`TFGPT2DoubleHeadsModel`].\n","\n","            Whether or not to add a projection after the vector extraction.\n","        summary_activation (`str`, *optional*):\n","            Argument used when doing sequence summary. Used in for the multiple choice head in\n","            [`GPT2DoubleHeadsModel`].\n","\n","            Pass `\"tanh\"` for a tanh activation to the output, any other value will result in no activation.\n","        summary_proj_to_labels (`bool`, *optional*, defaults to `True`):\n","            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n","            [`TFGPT2DoubleHeadsModel`].\n","\n","            Whether the projection outputs should have `config.num_labels` or `config.hidden_size` classes.\n","        summary_first_dropout (`float`, *optional*, defaults to 0.1):\n","            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n","            [`TFGPT2DoubleHeadsModel`].\n","\n","            The dropout ratio to be used after the projection and activation.\n","        scale_attn_weights (`bool`, *optional*, defaults to `True`):\n","            Scale attention weights by dividing by sqrt(hidden_size)..\n","        use_cache (`bool`, *optional*, defaults to `True`):\n","            Whether or not the model should return the last key/values attentions (not used by all models).\n","        scale_attn_by_inverse_layer_idx (`bool`, *optional*, defaults to `False`):\n","            Whether to additionally scale attention weights by `1 / layer_idx + 1`.\n","        reorder_and_upcast_attn (`bool`, *optional*, defaults to `False`):\n","            Whether to scale keys (K) prior to computing attention (dot-product) and upcast attention\n","            dot-product/softmax to float() when training with mixed precision.\n","\n","    Example:\n","\n","    ```python\n","    >>> from transformers import GPT2Config, GPT2Model\n","\n","    >>> # Initializing a GPT2 configuration\n","    >>> configuration = GPT2Config()\n","\n","    >>> # Initializing a model (with random weights) from the configuration\n","    >>> model = GPT2Model(configuration)\n","\n","    >>> # Accessing the model configuration\n","    >>> configuration = model.config\n","    ```\"\"\"\n","\n","    model_type = \"gpt2\"\n","    keys_to_ignore_at_inference = [\"past_key_values\"]\n","    attribute_map = {\n","        \"hidden_size\": \"n_embd\",\n","        \"max_position_embeddings\": \"n_positions\",\n","        \"num_attention_heads\": \"n_head\",\n","        \"num_hidden_layers\": \"n_layer\",\n","    }\n","\n","    def __init__(\n","        self,\n","        vocab_size=50257,\n","        n_positions=1024,\n","        n_embd=1024,\n","        n_layer=24,\n","        n_head=16,\n","        n_inner=None,\n","        activation_function=\"gelu_new\",\n","        resid_pdrop=0.1,\n","        embd_pdrop=0.1,\n","        attn_pdrop=0.1,\n","        layer_norm_epsilon=1e-5,\n","        initializer_range=0.02,\n","        summary_type=\"cls_index\",\n","        summary_use_proj=True,\n","        summary_activation=None,\n","        summary_proj_to_labels=True,\n","        summary_first_dropout=0.1,\n","        scale_attn_weights=True,\n","        use_cache=True,\n","        bos_token_id=50256,\n","        eos_token_id=50256,\n","        scale_attn_by_inverse_layer_idx=False,\n","        reorder_and_upcast_attn=False,\n","        **kwargs,\n","    ):\n","        self.vocab_size = vocab_size\n","        self.n_positions = n_positions\n","        self.n_embd = n_embd\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.n_inner = n_inner\n","        self.activation_function = activation_function\n","        self.resid_pdrop = resid_pdrop\n","        self.embd_pdrop = embd_pdrop\n","        self.attn_pdrop = attn_pdrop\n","        self.layer_norm_epsilon = layer_norm_epsilon\n","        self.initializer_range = initializer_range\n","        self.summary_type = summary_type\n","        self.summary_use_proj = summary_use_proj\n","        self.summary_activation = summary_activation\n","        self.summary_first_dropout = summary_first_dropout\n","        self.summary_proj_to_labels = summary_proj_to_labels\n","        self.scale_attn_weights = scale_attn_weights\n","        self.use_cache = use_cache\n","        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n","        self.reorder_and_upcast_attn = reorder_and_upcast_attn\n","\n","        self.bos_token_id = bos_token_id\n","        self.eos_token_id = eos_token_id\n","\n","        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n","\n"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["class ModuleUtilsMixin:\n","    \"\"\"\n","    A few utilities for `torch.nn.Modules`, to be used as a mixin.\n","    \"\"\"\n","\n","    @staticmethod\n","    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n","        try:\n","            import psutil\n","        except ImportError:\n","            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n","\n","        process = psutil.Process(os.getpid())\n","        mem = process.memory_info()\n","        module.mem_rss_pre_forward = mem.rss\n","        return None\n","\n","    @staticmethod\n","    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n","        try:\n","            import psutil\n","        except ImportError:\n","            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n","\n","        process = psutil.Process(os.getpid())\n","        mem = process.memory_info()\n","        module.mem_rss_post_forward = mem.rss\n","        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n","        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n","        return None\n","\n","    def add_memory_hooks(self):\n","        \"\"\"\n","        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n","\n","        Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n","        with `model.reset_memory_hooks_state()`.\n","        \"\"\"\n","        for module in self.modules():\n","            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n","            module.register_forward_hook(self._hook_rss_memory_post_forward)\n","        self.reset_memory_hooks_state()\n","\n","    def reset_memory_hooks_state(self):\n","        \"\"\"\n","        Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n","        \"\"\"\n","        for module in self.modules():\n","            module.mem_rss_diff = 0\n","            module.mem_rss_post_forward = 0\n","            module.mem_rss_pre_forward = 0\n","\n","    # @property\n","    # def device(self) -> torch.device:\n","    #     \"\"\"\n","    #     `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n","    #     device).\n","    #     \"\"\"\n","    #     return get_parameter_device(self)\n","\n","    # @property\n","    # def dtype(self) -> torch.dtype:\n","    #     \"\"\"\n","    #     `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n","    #     \"\"\"\n","    #     return get_parameter_dtype(self)\n","\n","    def invert_attention_mask(self, encoder_attention_mask: jt.array) -> jt.array:\n","        \"\"\"\n","        Invert an attention mask (e.g., switches 0. and 1.).\n","\n","        Args:\n","            encoder_attention_mask (`jt.array`): An attention mask.\n","\n","        Returns:\n","            `jt.array`: The inverted attention mask.\n","        \"\"\"\n","        if encoder_attention_mask.dim() == 3:\n","            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n","        if encoder_attention_mask.dim() == 2:\n","            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n","        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n","        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n","        # /transformer/transformer_layers.py#L270\n","        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n","        # encoder_extended_attention_mask.transpose(-1, -2))\n","        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n","        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * torch.finfo(self.dtype).min\n","\n","        return encoder_extended_attention_mask\n","\n","    @staticmethod\n","    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):\n","        if device is not None:\n","            warnings.warn(\n","                \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n","            )\n","        else:\n","            device = attention_mask.device\n","        batch_size, seq_length = input_shape\n","        seq_ids = torch.arange(seq_length, device=device)\n","        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n","        # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n","        # causal and attention masks must have same type with pytorch version < 1.3\n","        causal_mask = causal_mask.to(attention_mask.dtype)\n","\n","        if causal_mask.shape[1] < attention_mask.shape[1]:\n","            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n","            causal_mask = torch.cat(\n","                [\n","                    torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n","                    causal_mask,\n","                ],\n","                axis=-1,\n","            )\n","\n","        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n","        return extended_attention_mask\n","\n","    def get_extended_attention_mask(\n","        self, attention_mask: jt.array, input_shape: Tuple[int]\n","    ) -> jt.array:\n","        \"\"\"\n","        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n","\n","        Arguments:\n","            attention_mask (`jt.array`):\n","                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n","            input_shape (`Tuple[int]`):\n","                The shape of the input to the model.\n","\n","        Returns:\n","            `jt.array` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n","        \"\"\"\n","        # if dtype is None:\n","        #     dtype = self.dtype\n","\n","        # if not (attention_mask.dim() == 2 and self.config.is_decoder):\n","            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n","            # if device is not None:\n","            #     warnings.warn(\n","            #         \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n","            #     )\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        if attention_mask.dim() == 3:\n","            extended_attention_mask = attention_mask[:, None, :, :]\n","        elif attention_mask.dim() == 2:\n","            # Provided a padding mask of dimensions [batch_size, seq_length]\n","            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n","            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","            if self.config.is_decoder:\n","                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n","                    input_shape, attention_mask\n","                )\n","            else:\n","                extended_attention_mask = attention_mask[:, None, None, :]\n","        else:\n","            raise ValueError(\n","                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n","            )\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and the dtype's smallest value for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n","        return extended_attention_mask\n","\n","    def get_head_mask(\n","        self, head_mask: Optional[jt.array], num_hidden_layers: int, is_attention_chunked: bool = False\n","    ) -> jt.array:\n","        \"\"\"\n","        Prepare the head mask if needed.\n","\n","        Args:\n","            head_mask (`jt.array` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n","                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n","            num_hidden_layers (`int`):\n","                The number of hidden layers in the model.\n","            is_attention_chunked: (`bool`, *optional*, defaults to `False`):\n","                Whether or not the attentions scores are computed by chunks or not.\n","\n","        Returns:\n","            `jt.array` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n","            `[None]` for each layer.\n","        \"\"\"\n","        if head_mask is not None:\n","            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n","            if is_attention_chunked is True:\n","                head_mask = head_mask.unsqueeze(-1)\n","        else:\n","            head_mask = [None] * num_hidden_layers\n","\n","        return head_mask\n","\n","    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n","        \"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\n","        if head_mask.dim() == 1:\n","            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n","        elif head_mask.dim() == 2:\n","            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n","        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n","        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n","        return head_mask\n","\n","    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n","        \"\"\"\n","        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n","\n","        Args:\n","            only_trainable (`bool`, *optional*, defaults to `False`):\n","                Whether or not to return only the number of trainable parameters\n","\n","            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n","                Whether or not to return only the number of non-embeddings parameters\n","\n","        Returns:\n","            `int`: The number of parameters.\n","        \"\"\"\n","\n","        if exclude_embeddings:\n","            embedding_param_names = [\n","                f\"{name}.weight\" for name, module_type in self.named_modules() if isinstance(module_type, nn.Embedding)\n","            ]\n","            non_embedding_parameters = [\n","                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n","            ]\n","            return sum(p.numel() for p in non_embedding_parameters if p.requires_grad or not only_trainable)\n","        else:\n","            return sum(p.numel() for p in self.parameters() if p.requires_grad or not only_trainable)\n","\n","    def estimate_tokens(self, input_dict: Dict[str, Union[jt.array, Any]]) -> int:\n","        \"\"\"\n","        Helper function to estimate the total number of tokens from the model inputs.\n","\n","        Args:\n","            inputs (`dict`): The model inputs.\n","\n","        Returns:\n","            `int`: The total number of tokens.\n","        \"\"\"\n","        if not hasattr(self, \"warnings_issued\"):\n","            self.warnings_issued = {}\n","        if self.main_input_name in input_dict:\n","            return input_dict[self.main_input_name].numel()\n","        elif \"estimate_tokens\" not in self.warnings_issued:\n","            logger.warning(\n","                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n","            )\n","            self.warnings_issued[\"estimate_tokens\"] = True\n","        return 0\n","\n","    def floating_point_ops(\n","        self, input_dict: Dict[str, Union[jt.array, Any]], exclude_embeddings: bool = True\n","    ) -> int:\n","        \"\"\"\n","        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n","        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n","        tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n","        paper](https://arxiv.org/pdf/2001.08361.pdf) section 2.1. Should be overridden for transformers with parameter\n","        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n","\n","        Args:\n","            batch_size (`int`):\n","                The batch size for the forward pass.\n","\n","            sequence_length (`int`):\n","                The number of tokens in each line of the batch.\n","\n","            exclude_embeddings (`bool`, *optional*, defaults to `True`):\n","                Whether or not to count embedding and softmax operations.\n","\n","        Returns:\n","            `int`: The number of floating-point operations.\n","        \"\"\"\n","\n","        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n","\n"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["class PreTrainedModel(nn.Module, ModuleUtilsMixin):\n","    r\"\"\"\n","    Base class for all models.\n","\n","    [`PreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,\n","    downloading and saving models as well as a few methods common to all models to:\n","\n","        - resize the input embeddings,\n","        - prune heads in the self-attention heads.\n","\n","    Class attributes (overridden by derived classes):\n","\n","        - **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class\n","          for this model architecture.\n","        - **load_tf_weights** (`Callable`) -- A python *method* for loading a TensorFlow checkpoint in a Jittor model,\n","          taking as arguments:\n","\n","            - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.\n","            - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.\n","            - **path** (`str`) -- A path to the TensorFlow checkpoint.\n","\n","        - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n","          classes of the same architecture adding modules on top of the base model.\n","        - **is_parallelizable** (`bool`) -- A flag indicating whether this model supports model parallelization.\n","        - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n","          models, `pixel_values` for vision models and `input_values` for speech models).\n","    \"\"\"\n","    config_class = None\n","    base_model_prefix = \"\"\n","    main_input_name = \"input_ids\"\n","    _auto_class = None\n","    _no_split_modules = None\n","\n","    # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n","    # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n","    _keys_to_ignore_on_load_missing = None\n","    # a list of `re` patterns of `state_dict` keys that should be removed from the list of\n","    # unexpected keys we find (keys inside the checkpoint but not the model) and avoid unnecessary\n","    # warnings.\n","    _keys_to_ignore_on_load_unexpected = None\n","    # a list of `state_dict` keys to ignore when saving the model (useful for keys that aren't\n","    # trained, but which are either deterministic or tied variables)\n","    _keys_to_ignore_on_save = None\n","\n","    is_parallelizable = False\n","    supports_gradient_checkpointing = False\n","\n","    # @property\n","    # def dummy_inputs(self) -> Dict[str, jt.array]:\n","    #     \"\"\"\n","    #     `Dict[str, jt.array]`: Dummy inputs to do a forward pass in the network.\n","    #     \"\"\"\n","    #     return {\"input_ids\": jt.array(DUMMY_INPUTS)}\n","\n","    @property\n","    def framework(self) -> str:\n","        \"\"\"\n","        :str: Identifies that this is a PyTorch model.\n","        \"\"\"\n","        return \"pt\"\n","\n","    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n","        super().__init__()\n","        if not isinstance(config, PretrainedConfig):\n","            raise ValueError(\n","                f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n","                \"`PretrainedConfig`. To create a model from a pretrained model use \"\n","                f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n","            )\n","        # Save config and origin of the pretrained weights if given in model\n","        self.config = config\n","        self.name_or_path = config.name_or_path\n","        self.warnings_issued = {}\n","\n","    def post_init(self):\n","        \"\"\"\n","        A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n","        modules properly initialized (such as weight initialization).\n","        \"\"\"\n","        pass\n","        # self.init_weights()\n","        # self._backward_compatibility_gradient_checkpointing()\n","\n","    def _backward_compatibility_gradient_checkpointing(self):\n","        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n","            self.gradient_checkpointing_enable()\n","            # Remove the attribute now that is has been consumed, so it's no saved in the config.\n","            delattr(self.config, \"gradient_checkpointing\")\n","\n","    @classmethod\n","    def _from_config(cls, config, **kwargs):\n","        \"\"\"\n","        All context managers that the model should be initialized under go here.\n","\n","        Args:\n","            torch_dtype (`torch.dtype`, *optional*):\n","                Override the default `torch.dtype` and load the model under this dtype.\n","        \"\"\"\n","        jt_dtype = kwargs.pop(\"torch_dtype\", None)\n","\n","        # override default dtype if needed\n","        dtype_orig = None\n","        if jt_dtype is not None:\n","            dtype_orig = cls._set_default_torch_dtype(jt_dtype)\n","\n","        # if is_deepspeed_zero3_enabled():\n","        #     import deepspeed\n","\n","        #     logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n","        #     # this immediately partitions the model across all gpus, to avoid the overhead in time\n","        #     # and memory copying it on CPU or each GPU first\n","        #     with deepspeed.zero.Init(config_dict_or_path=deepspeed_config()):\n","        #         model = cls(config, **kwargs)\n","        # else:\n","        model = cls(config, **kwargs)\n","\n","        # restore default dtype if it was modified\n","        # TODO\n","        # if dtype_orig is not None:\n","        #     torch.set_default_dtype(dtype_orig)\n","\n","        return model\n","    \n","    @property\n","    def base_model(self) -> nn.Module:\n","        \"\"\"\n","        `jittor.nn.Module`: The main body of the model.\n","        \"\"\"\n","        return getattr(self, self.base_model_prefix, self)\n","\n","    def get_input_embeddings(self) -> nn.Module:\n","        \"\"\"\n","        Returns the model's input embeddings.\n","\n","        Returns:\n","            `nn.Module`: A torch module mapping vocabulary to hidden states.\n","        \"\"\"\n","        base_model = getattr(self, self.base_model_prefix, self)\n","        if base_model is not self:\n","            return base_model.get_input_embeddings()\n","        else:\n","            raise NotImplementedError\n","\n","    def get_output_embeddings(self) -> nn.Module:\n","        \"\"\"\n","        Returns the model's output embeddings.\n","\n","        Returns:\n","            `nn.Module`: A jittor module mapping hidden states to vocabulary.\n","        \"\"\"\n","        return None  # Overwrite for models with output embeddings\n","\n","    def _init_weights(self, module):\n","        \"\"\"\n","        Initialize the weights. This method should be overridden by derived class.\n","        \"\"\"\n","        raise NotImplementedError(f\"Make sure `_init_weights` is implemented for {self.__class__}\")\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n","        r\"\"\"\n","        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n","\n","        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n","        the model, you should first set it back in training mode with `model.train()`.\n","\n","        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n","        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n","        task.\n","\n","        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n","        weights are discarded.\n","\n","        Parameters:\n","            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n","                Can be either:\n","\n","                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n","                      Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n","                      user or organization name, like `dbmdz/bert-base-german-cased`.\n","                    - A path to a *directory* containing model weights saved using\n","                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n","                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n","                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n","                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n","                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n","                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n","                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n","                      `True`.\n","                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n","                      arguments `config` and `state_dict`).\n","            model_args (sequence of positional arguments, *optional*):\n","                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n","            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n","                Can be either:\n","\n","                    - an instance of a class derived from [`PretrainedConfig`],\n","                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n","\n","                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n","                be automatically loaded when:\n","\n","                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n","                      model).\n","                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n","                      save directory.\n","                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n","                      configuration JSON file named *config.json* is found in the directory.\n","            state_dict (`Dict[str, torch.Tensor]`, *optional*):\n","                A state dictionary to use instead of a state dictionary loaded from saved weights file.\n","\n","                This option can be used if you want to create a model from a pretrained configuration but load your own\n","                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n","                [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n","            cache_dir (`Union[str, os.PathLike]`, *optional*):\n","                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n","                standard cache should not be used.\n","            from_tf (`bool`, *optional*, defaults to `False`):\n","                Load the model weights from a TensorFlow checkpoint save file (see docstring of\n","                `pretrained_model_name_or_path` argument).\n","            from_flax (`bool`, *optional*, defaults to `False`):\n","                Load the model weights from a Flax checkpoint save file (see docstring of\n","                `pretrained_model_name_or_path` argument).\n","            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n","                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n","                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n","                checkpoint with 3 labels).\n","            force_download (`bool`, *optional*, defaults to `False`):\n","                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n","                cached versions if they exist.\n","            resume_download (`bool`, *optional*, defaults to `False`):\n","                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n","                file exists.\n","            proxies (`Dict[str, str]`, *optional*):\n","                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n","                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n","            output_loading_info(`bool`, *optional*, defaults to `False`):\n","                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n","            local_files_only(`bool`, *optional*, defaults to `False`):\n","                Whether or not to only look at local files (i.e., do not try to download the model).\n","            use_auth_token (`str` or `bool`, *optional*):\n","                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n","                the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n","            revision (`str`, *optional*, defaults to `\"main\"`):\n","                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n","                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n","                identifier allowed by git.\n","\n","\n","                <Tip>\n","\n","                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n","\n","                </Tip>\n","\n","            mirror (`str`, *optional*):\n","                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n","                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n","                Please refer to the mirror site for more information.\n","            _fast_init(`bool`, *optional*, defaults to `True`):\n","                Whether or not to disable fast initialization.\n","\n","                <Tip warning={true}>\n","\n","                One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n","                4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n","                [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n","\n","                </Tip>\n","\n","            > Parameters for big model inference\n","\n","            low_cpu_mem_usage(`bool`, *optional*):\n","                Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n","                This is an experimental feature and a subject to change at any moment.\n","            torch_dtype (`str` or `torch.dtype`, *optional*):\n","                Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n","                will be automatically derived from the model's weights.\n","            device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n","                A map that specifies where each submodule should go. It doesn't need to be refined to each\n","                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n","                same device.\n","\n","                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n","                more information about each option see [designing a device\n","                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n","            max_memory (`Dict`, *optional*):\n","                A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n","                GPU and the available CPU RAM if unset.\n","            offload_folder (`str` or `os.PathLike`, *optional*):\n","                If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n","            offload_state_dict (`bool`, *optional*):\n","                If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n","                RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n","                `True` when there is some disk offload.\n","            load_in_8bit (`bool`, *optional*, defaults to `False`):\n","                If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\n","                install `bitsandbytes` compiled with your CUDA version by running `pip install -i\n","                https://test.pypi.org/simple/ bitsandbytes-cudaXXX` where XXX is your CUDA version (e.g. 11.6 = 116).\n","                Make also sure that you have enough GPU RAM to store half of the model size since the 8bit modules are\n","                not compiled and adapted for CPUs.\n","            load_in_8bit_threshold (`float`, *optional*, defaults to 6):\n","                Works together with `load_in_8bit`. This corresponds to the outlier threshold for outlier detection as\n","                described in `GPT3.int8() : 8-bit Matrix Multiplication for Transformers at Scale` paper. Any hidden\n","                states value that is above this threshold will be considered an outlier and the operation on those\n","                values will be done in fp16. Values are usually normally distributed, that is, most values are in the\n","                range [-3.5, 3.5], but there are some exceptional systematic outliers that are very differently\n","                distributed for large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8\n","                quantization works well for values of magnitude ~5, but beyond that, there is a significant performance\n","                penalty. A good default threshold is 6, but a lower threshold might be needed for more unstable models\n","                (small models, fine-tuning).\n","            load_in_8bit_skip_modules (`List[str]`, *optional*):\n","                An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such\n","                as Jukebox that has several heads in different places and not necessarily at the last position.\n","            subfolder (`str`, *optional*, defaults to `\"\"`):\n","                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n","                specify the folder name here.\n","\n","            kwargs (remaining dictionary of keyword arguments, *optional*):\n","                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n","                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n","                automatically loaded:\n","\n","                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n","                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n","                      already been done)\n","                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n","                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n","                      corresponds to a configuration attribute will be used to override said attribute with the\n","                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n","                      will be passed to the underlying model's `__init__` function.\n","\n","        <Tip>\n","\n","        Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n","        use this method in a firewalled environment.\n","\n","        </Tip>\n","\n","        Examples:\n","\n","        ```python\n","        >>> from transformers import BertConfig, BertModel\n","\n","        >>> # Download model and configuration from huggingface.co and cache.\n","        >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n","        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n","        >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n","        >>> # Update configuration during loading.\n","        >>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n","        >>> assert model.config.output_attentions == True\n","        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n","        >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n","        >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n","        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n","        >>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n","        ```\n","\n","        * `low_cpu_mem_usage` algorithm:\n","\n","        This is an experimental function that loads the model using ~1x model size CPU memory\n","\n","        Here is how it works:\n","\n","        1. save which state_dict keys we have\n","        2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n","        3. after the model has been instantiated switch to the meta device all params/buffers that\n","        are going to be replaced from the loaded state_dict\n","        4. load state_dict 2nd time\n","        5. replace the params/buffers from the state_dict\n","\n","        Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n","\n","        \"\"\"\n","        config = kwargs.pop(\"config\", None)\n","        state_dict = kwargs.pop(\"state_dict\", None)\n","        cache_dir = kwargs.pop(\"cache_dir\", None)\n","        from_tf = kwargs.pop(\"from_tf\", False)\n","        from_flax = kwargs.pop(\"from_flax\", False)\n","        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n","        force_download = kwargs.pop(\"force_download\", False)\n","        resume_download = kwargs.pop(\"resume_download\", False)\n","        proxies = kwargs.pop(\"proxies\", None)\n","        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n","        local_files_only = kwargs.pop(\"local_files_only\", False)\n","        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n","        revision = kwargs.pop(\"revision\", None)\n","        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n","        _ = kwargs.pop(\"mirror\", None)\n","        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n","        from_auto_class = kwargs.pop(\"_from_auto\", False)\n","        _fast_init = kwargs.pop(\"_fast_init\", True)\n","        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n","        low_cpu_mem_usage = kwargs.pop(\"low_cpu_mem_usage\", None)\n","        device_map = kwargs.pop(\"device_map\", None)\n","        max_memory = kwargs.pop(\"max_memory\", None)\n","        offload_folder = kwargs.pop(\"offload_folder\", None)\n","        offload_state_dict = kwargs.pop(\"offload_state_dict\", False)\n","        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n","        load_in_8bit_threshold = kwargs.pop(\"load_in_8bit_threshold\", 6.0)\n","        load_in_8bit_skip_modules = kwargs.pop(\"load_in_8bit_skip_modules\", None)\n","        subfolder = kwargs.pop(\"subfolder\", \"\")\n","        commit_hash = kwargs.pop(\"_commit_hash\", None)\n","\n","        if trust_remote_code is True:\n","            logger.warning(\n","                \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\"\n","                \" ignored.\"\n","            )\n","        if device_map is not None:\n","            if low_cpu_mem_usage is None:\n","                low_cpu_mem_usage = True\n","            elif not low_cpu_mem_usage:\n","                raise ValueError(\"Passing along a `device_map` requires `low_cpu_mem_usage=True`\")\n","\n","        if low_cpu_mem_usage:\n","            # low_cpu_mem_usage requires PyTorch >= 1.9 to have the meta device.\n","            require_version_core(\"torch>=1.9\")\n","            if device_map is not None:\n","                # The max memory utils require PyTorch >= 1.10 to have torch.cuda.mem_get_info.\n","                require_version_core(\"torch>=1.10\")\n","\n","            if is_deepspeed_zero3_enabled():\n","                raise ValueError(\n","                    \"DeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\"\n","                )\n","            elif not is_accelerate_available():\n","                raise ImportError(\n","                    \"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\"\n","                )\n","\n","        if load_in_8bit:\n","            if not (is_accelerate_available() and is_bitsandbytes_available()):\n","                raise ImportError(\n","                    \"Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\"\n","                    \" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\"\n","                    \" pip install bitsandbytes` \"\n","                )\n","            if torch_dtype == \"auto\" or torch_dtype != torch.float16:\n","                # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n","                torch_dtype = torch.float16\n","                logger.info(\"Loading the model in mixed int8 - forcing the weights to be casted in float16\")\n","            if device_map is None:\n","                raise ValueError(\n","                    \"A device map needs to be passed to run convert models into mixed-int8 format. Please run\"\n","                    \"`.from_pretrained` with `device_map='auto'`\"\n","                )\n","            if from_tf or from_flax:\n","                raise ValueError(\n","                    \"Converting into mixed 8-bit weights from tf/flax weights is currently not supported, please make\"\n","                    \" sure the weights are in PyTorch format.\"\n","                )\n","\n","        from_pt = not (from_tf | from_flax)\n","\n","        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n","        if from_pipeline is not None:\n","            user_agent[\"using_pipeline\"] = from_pipeline\n","\n","        if is_offline_mode() and not local_files_only:\n","            logger.info(\"Offline mode: forcing local_files_only=True\")\n","            local_files_only = True\n","\n","        # Load config if we don't provide a configuration\n","        if not isinstance(config, PretrainedConfig):\n","            config_path = config if config is not None else pretrained_model_name_or_path\n","            config, model_kwargs = cls.config_class.from_pretrained(\n","                config_path,\n","                cache_dir=cache_dir,\n","                return_unused_kwargs=True,\n","                force_download=force_download,\n","                resume_download=resume_download,\n","                proxies=proxies,\n","                local_files_only=local_files_only,\n","                use_auth_token=use_auth_token,\n","                revision=revision,\n","                subfolder=subfolder,\n","                _from_auto=from_auto_class,\n","                _from_pipeline=from_pipeline,\n","                **kwargs,\n","            )\n","        else:\n","            model_kwargs = kwargs\n","\n","        if commit_hash is None:\n","            commit_hash = getattr(config, \"_commit_hash\", None)\n","\n","        # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the\n","        # index of the files.\n","        is_sharded = False\n","        sharded_metadata = None\n","        # Load model\n","        loading_info = None\n","\n","        if pretrained_model_name_or_path is not None:\n","            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n","            is_local = os.path.isdir(pretrained_model_name_or_path)\n","            if is_local:\n","                if from_tf and os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n","                ):\n","                    # Load from a TF 1.0 checkpoint in priority if from_tf\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n","                elif from_tf and os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)\n","                ):\n","                    # Load from a TF 2.0 checkpoint in priority if from_tf\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)\n","                elif from_flax and os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n","                ):\n","                    # Load from a Flax checkpoint in priority if from_flax\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n","                elif is_safetensors_available() and os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_NAME)\n","                ):\n","                    # Load from a safetensors checkpoint\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_NAME)\n","                elif is_safetensors_available() and os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_INDEX_NAME)\n","                ):\n","                    # Load from a sharded safetensors checkpoint\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_INDEX_NAME)\n","                    is_sharded = True\n","                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)):\n","                    # Load from a PyTorch checkpoint\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n","                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_INDEX_NAME)):\n","                    # Load from a sharded PyTorch checkpoint\n","                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_INDEX_NAME)\n","                    is_sharded = True\n","                # At this stage we don't have a weight file so we will raise an error.\n","                elif os.path.isfile(\n","                    os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n","                ) or os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)):\n","                    raise EnvironmentError(\n","                        f\"Error no file named {WEIGHTS_NAME} found in directory {pretrained_model_name_or_path} but \"\n","                        \"there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those \"\n","                        \"weights.\"\n","                    )\n","                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)):\n","                    raise EnvironmentError(\n","                        f\"Error no file named {WEIGHTS_NAME} found in directory {pretrained_model_name_or_path} but \"\n","                        \"there is a file for Flax weights. Use `from_flax=True` to load this model from those \"\n","                        \"weights.\"\n","                    )\n","                else:\n","                    raise EnvironmentError(\n","                        f\"Error no file named {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or \"\n","                        f\"{FLAX_WEIGHTS_NAME} found in directory {pretrained_model_name_or_path}.\"\n","                    )\n","            elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n","                archive_file = pretrained_model_name_or_path\n","                is_local = True\n","            elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path + \".index\")):\n","                if not from_tf:\n","                    raise ValueError(\n","                        f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n","                        \"from_tf to True to load from this checkpoint.\"\n","                    )\n","                archive_file = os.path.join(subfolder, pretrained_model_name_or_path + \".index\")\n","                is_local = True\n","            elif is_remote_url(pretrained_model_name_or_path):\n","                filename = pretrained_model_name_or_path\n","                resolved_archive_file = download_url(pretrained_model_name_or_path)\n","            else:\n","                # set correct filename\n","                if from_tf:\n","                    filename = TF2_WEIGHTS_NAME\n","                elif from_flax:\n","                    filename = FLAX_WEIGHTS_NAME\n","                elif is_safetensors_available():\n","                    filename = SAFE_WEIGHTS_NAME\n","                else:\n","                    filename = WEIGHTS_NAME\n","\n","                try:\n","                    # Load from URL or cache if already cached\n","                    cached_file_kwargs = dict(\n","                        cache_dir=cache_dir,\n","                        force_download=force_download,\n","                        proxies=proxies,\n","                        resume_download=resume_download,\n","                        local_files_only=local_files_only,\n","                        use_auth_token=use_auth_token,\n","                        user_agent=user_agent,\n","                        revision=revision,\n","                        subfolder=subfolder,\n","                        _raise_exceptions_for_missing_entries=False,\n","                        _commit_hash=commit_hash,\n","                    )\n","                    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n","\n","                    # Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\n","                    # result when internet is up, the repo and revision exist, but the file does not.\n","                    if resolved_archive_file is None and filename == SAFE_WEIGHTS_NAME:\n","                        # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n","                        resolved_archive_file = cached_file(\n","                            pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **cached_file_kwargs\n","                        )\n","                        if resolved_archive_file is not None:\n","                            is_sharded = True\n","                        else:\n","                            # This repo has no safetensors file of any kind, we switch to PyTorch.\n","                            filename = WEIGHTS_NAME\n","                            resolved_archive_file = cached_file(\n","                                pretrained_model_name_or_path, WEIGHTS_NAME, **cached_file_kwargs\n","                            )\n","                    if resolved_archive_file is None and filename == WEIGHTS_NAME:\n","                        # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n","                        resolved_archive_file = cached_file(\n","                            pretrained_model_name_or_path, WEIGHTS_INDEX_NAME, **cached_file_kwargs\n","                        )\n","                        if resolved_archive_file is not None:\n","                            is_sharded = True\n","                    if resolved_archive_file is None:\n","                        # Otherwise, maybe there is a TF or Flax model file.  We try those to give a helpful error\n","                        # message.\n","                        has_file_kwargs = {\n","                            \"revision\": revision,\n","                            \"proxies\": proxies,\n","                            \"use_auth_token\": use_auth_token,\n","                        }\n","                        if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n","                            raise EnvironmentError(\n","                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n","                                f\" {WEIGHTS_NAME} but there is a file for TensorFlow weights. Use `from_tf=True` to\"\n","                                \" load this model from those weights.\"\n","                            )\n","                        elif has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):\n","                            raise EnvironmentError(\n","                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n","                                f\" {WEIGHTS_NAME} but there is a file for Flax weights. Use `from_flax=True` to load\"\n","                                \" this model from those weights.\"\n","                            )\n","                        else:\n","                            raise EnvironmentError(\n","                                f\"{pretrained_model_name_or_path} does not appear to have a file named {WEIGHTS_NAME},\"\n","                                f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}.\"\n","                            )\n","                except EnvironmentError:\n","                    # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\n","                    # to the original exception.\n","                    raise\n","                except Exception:\n","                    # For any other exception, we throw a generic error.\n","                    raise EnvironmentError(\n","                        f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it\"\n","                        \" from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n","                        f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n","                        f\" directory containing a file named {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or\"\n","                        f\" {FLAX_WEIGHTS_NAME}.\"\n","                    )\n","\n","            if is_local:\n","                logger.info(f\"loading weights file {archive_file}\")\n","                resolved_archive_file = archive_file\n","            else:\n","                logger.info(f\"loading weights file {filename} from cache at {resolved_archive_file}\")\n","        else:\n","            resolved_archive_file = None\n","\n","        # We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\n","        if is_sharded:\n","            # rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\n","            resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n","                pretrained_model_name_or_path,\n","                resolved_archive_file,\n","                cache_dir=cache_dir,\n","                force_download=force_download,\n","                proxies=proxies,\n","                resume_download=resume_download,\n","                local_files_only=local_files_only,\n","                use_auth_token=use_auth_token,\n","                user_agent=user_agent,\n","                revision=revision,\n","                subfolder=subfolder,\n","                _commit_hash=commit_hash,\n","            )\n","\n","        # load pt weights early so that we know which dtype to init the model under\n","        if from_pt:\n","            if not is_sharded and state_dict is None:\n","                # Time to load the checkpoint\n","                state_dict = load_state_dict(resolved_archive_file)\n","\n","            # set dtype to instantiate the model under:\n","            # 1. If torch_dtype is not None, we use that dtype\n","            # 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n","            #    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\n","            # we also may have config.torch_dtype available, but we won't rely on it till v5\n","            dtype_orig = None\n","            if torch_dtype is not None:\n","                if isinstance(torch_dtype, str):\n","                    if torch_dtype == \"auto\":\n","                        if is_sharded and \"dtype\" in sharded_metadata:\n","                            torch_dtype = sharded_metadata[\"dtype\"]\n","                        elif not is_sharded:\n","                            torch_dtype = get_state_dict_dtype(state_dict)\n","                        else:\n","                            one_state_dict = load_state_dict(resolved_archive_file[0])\n","                            torch_dtype = get_state_dict_dtype(one_state_dict)\n","                            del one_state_dict  # free CPU memory\n","                    else:\n","                        raise ValueError(\n","                            f\"`torch_dtype` can be either a `torch.dtype` or `auto`, but received {torch_dtype}\"\n","                        )\n","                dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n","\n","            if is_sharded:\n","                loaded_state_dict_keys = sharded_metadata[\"all_checkpoint_keys\"]\n","            else:\n","                loaded_state_dict_keys = [k for k in state_dict.keys()]\n","            if low_cpu_mem_usage:\n","                state_dict = None\n","\n","        config.name_or_path = pretrained_model_name_or_path\n","\n","        # Instantiate model.\n","        init_contexts = [no_init_weights(_enable=_fast_init)]\n","\n","        if is_deepspeed_zero3_enabled():\n","            import deepspeed\n","\n","            logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n","            init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())] + init_contexts\n","        elif load_in_8bit or low_cpu_mem_usage:\n","            init_contexts.append(init_empty_weights())\n","\n","        with ContextManagers(init_contexts):\n","            model = cls(config, *model_args, **model_kwargs)\n","\n","        if load_in_8bit:\n","            from .utils.bitsandbytes import get_keys_to_not_convert, replace_8bit_linear\n","\n","            logger.info(\"Detected 8-bit loading: activating 8-bit loading for this model\")\n","\n","            # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n","            if load_in_8bit_skip_modules is None:\n","                modules_to_not_convert = get_keys_to_not_convert(model)\n","            else:\n","                modules_to_not_convert = load_in_8bit_skip_modules\n","            model = replace_8bit_linear(\n","                model, threshold=load_in_8bit_threshold, modules_to_not_convert=modules_to_not_convert\n","            )\n","\n","        if isinstance(device_map, str):\n","            if model._no_split_modules is None:\n","                raise ValueError(f\"{model.__class__.__name__} does not support `device_map='{device_map}'` yet.\")\n","            no_split_modules = model._no_split_modules\n","            if device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n","                raise ValueError(\n","                    \"If passing a string for `device_map`, please choose 'auto', 'balanced', 'balanced_low_0' or \"\n","                    \"'sequential'.\"\n","                )\n","            elif device_map in [\"balanced\", \"balanced_low_0\"] and get_balanced_memory is None:\n","                raise ValueError(f\"`device_map={device_map}` requires a source install of Accelerate.\")\n","            if device_map != \"sequential\" and get_balanced_memory is not None:\n","                max_memory = get_balanced_memory(\n","                    model,\n","                    max_memory=max_memory,\n","                    no_split_module_classes=no_split_modules,\n","                    dtype=torch_dtype,\n","                    low_zero=(device_map == \"balanced_low_0\"),\n","                )\n","            # Make sure tied weights are tied before creating the device map.\n","            model.tie_weights()\n","            device_map = infer_auto_device_map(\n","                model,\n","                no_split_module_classes=no_split_modules,\n","                dtype=torch_dtype if not load_in_8bit else torch.int8,\n","                max_memory=max_memory,\n","            )\n","\n","            if load_in_8bit:\n","                # The LM head / tied weights or any last module can stay on disk / CPU\n","                device_map_without_lm_head = {\n","                    key: device_map[key] for key in device_map.keys() if key not in modules_to_not_convert\n","                }\n","                if \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n","                    raise ValueError(\n","                        \"\"\"\n","                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n","                        the quantized model. If you have set a value for `max_memory` you should increase that. To have\n","                        an idea of the modules that are set on the CPU or RAM you can print model.hf_device_map.\n","                        \"\"\"\n","                    )\n","                del device_map_without_lm_head\n","\n","        if from_tf:\n","            if resolved_archive_file.endswith(\".index\"):\n","                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n","                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n","            else:\n","                # Load from our TensorFlow 2.0 checkpoints\n","                try:\n","                    from .modeling_tf_pytorch_utils import load_tf2_checkpoint_in_pytorch_model\n","\n","                    model, loading_info = load_tf2_checkpoint_in_pytorch_model(\n","                        model, resolved_archive_file, allow_missing_keys=True, output_loading_info=True\n","                    )\n","                except ImportError:\n","                    logger.error(\n","                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\"\n","                        \" Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\"\n","                        \" instructions.\"\n","                    )\n","                    raise\n","        elif from_flax:\n","            try:\n","                from .modeling_flax_pytorch_utils import load_flax_checkpoint_in_pytorch_model\n","\n","                model = load_flax_checkpoint_in_pytorch_model(model, resolved_archive_file)\n","            except ImportError:\n","                logger.error(\n","                    \"Loading a Flax model in PyTorch, requires both PyTorch and Flax to be installed. Please see\"\n","                    \" https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for\"\n","                    \" installation instructions.\"\n","                )\n","                raise\n","        elif from_pt:\n","\n","            # restore default dtype\n","            if dtype_orig is not None:\n","                torch.set_default_dtype(dtype_orig)\n","\n","            (\n","                model,\n","                missing_keys,\n","                unexpected_keys,\n","                mismatched_keys,\n","                offload_index,\n","                error_msgs,\n","            ) = cls._load_pretrained_model(\n","                model,\n","                state_dict,\n","                loaded_state_dict_keys,  # XXX: rename?\n","                resolved_archive_file,\n","                pretrained_model_name_or_path,\n","                ignore_mismatched_sizes=ignore_mismatched_sizes,\n","                sharded_metadata=sharded_metadata,\n","                _fast_init=_fast_init,\n","                low_cpu_mem_usage=low_cpu_mem_usage,\n","                device_map=device_map,\n","                offload_folder=offload_folder,\n","                offload_state_dict=offload_state_dict,\n","                dtype=torch_dtype,\n","                load_in_8bit=load_in_8bit,\n","            )\n","\n","        model.is_loaded_in_8bit = load_in_8bit\n","\n","        # make sure token embedding weights are still tied if needed\n","        model.tie_weights()\n","\n","        # Set model in evaluation mode to deactivate DropOut modules by default\n","        model.eval()\n","\n","        # Dispatch model with hooks on all devices if necessary\n","        if device_map is not None:\n","            dispatch_model(model, device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)\n","\n","        if output_loading_info:\n","            if loading_info is None:\n","                loading_info = {\n","                    \"missing_keys\": missing_keys,\n","                    \"unexpected_keys\": unexpected_keys,\n","                    \"mismatched_keys\": mismatched_keys,\n","                    \"error_msgs\": error_msgs,\n","                }\n","            return model, loading_info\n","\n","        return model\n","\n","\n"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["def load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n","    \"\"\"Load tf checkpoints in a pytorch model\"\"\"\n","    try:\n","        import re\n","\n","        import tensorflow as tf\n","    except ImportError:\n","        logger.error(\n","            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n","            \"https://www.tensorflow.org/install/ for installation instructions.\"\n","        )\n","        raise\n","    tf_path = os.path.abspath(gpt2_checkpoint_path)\n","    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n","    # Load weights from TF model\n","    init_vars = tf.train.list_variables(tf_path)\n","    names = []\n","    arrays = []\n","    for name, shape in init_vars:\n","        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n","        array = tf.train.load_variable(tf_path, name)\n","        names.append(name)\n","        arrays.append(array.squeeze())\n","\n","    for name, array in zip(names, arrays):\n","        name = name[6:]  # skip \"model/\"\n","        name = name.split(\"/\")\n","        pointer = model\n","        for m_name in name:\n","            if re.fullmatch(r\"[A-Za-z]+\\d+\", m_name):\n","                scope_names = re.split(r\"(\\d+)\", m_name)\n","            else:\n","                scope_names = [m_name]\n","            if scope_names[0] == \"w\" or scope_names[0] == \"g\":\n","                pointer = getattr(pointer, \"weight\")\n","            elif scope_names[0] == \"b\":\n","                pointer = getattr(pointer, \"bias\")\n","            elif scope_names[0] == \"wpe\" or scope_names[0] == \"wte\":\n","                pointer = getattr(pointer, scope_names[0])\n","                pointer = getattr(pointer, \"weight\")\n","            else:\n","                pointer = getattr(pointer, scope_names[0])\n","            if len(scope_names) >= 2:\n","                num = int(scope_names[1])\n","                pointer = pointer[num]\n","        try:\n","            assert (\n","                pointer.shape == array.shape\n","            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n","        except AssertionError as e:\n","            e.args += (pointer.shape, array.shape)\n","            raise\n","        logger.info(f\"Initialize PyTorch weight {name}\")\n","        pointer.data = torch.from_numpy(array)\n","    return model\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["class GPT2PreTrainedModel(PreTrainedModel):\n","    \"\"\"\n","    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n","    models.\n","    \"\"\"\n","    config_class = GPT2Config\n","    load_tf_weights = load_tf_weights_in_gpt2\n","    base_model_prefix = \"transformer\"\n","    is_parallelizable = True\n","    supports_gradient_checkpointing = True\n","    _no_split_modules = [\"GPT2Block\"]\n","\n","    def __init__(self, *inputs, **kwargs):\n","        super().__init__(*inputs, **kwargs)\n","    \n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights.\"\"\"\n","        if isinstance(module, (nn.Linear, Conv1D)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight = jt.normal(mean=0.0, std=self.config.initializer_range, size=module.weight.shape)\n","            if module.bias is not None:\n","                module.bias.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight = jt.normal(mean=0.0, std=self.config.initializer_range, size=module.weight.shape)\n","            # if module.padding_idx is not None:\n","            #     module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.zero_()\n","            module.weight = jt.full(module.weight.shape,1.0)\n","\n","        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n","        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n","        #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.\n","        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n","        #\n","        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n","        for name, p in module.named_parameters():\n","            if name == \"c_proj.weight\":\n","                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n","                p.data=jt.normal(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n","\n","    def _set_gradient_checkpointing(self, module, value=False):\n","        if isinstance(module, GPT2Model):\n","            module.gradient_checkpointing = value\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["GPT2_START_DOCSTRING = r\"\"\"\n","\n","    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n","    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n","    etc.)\n","\n","    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n","    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n","    and behavior.\n","\n","    Parameters:\n","        config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the\n","            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n","\"\"\"\n","\n","GPT2_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (`jt.array` of shape `(batch_size, input_ids_length)`):\n","            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n","            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n","            sequence tokens in the vocabulary.\n","\n","            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n","            `input_ids`.\n","\n","            Indices can be obtained using [`GPT2Tokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","            [`PreTrainedTokenizer.__call__`] for details.\n","\n","            [What are input IDs?](../glossary#input-ids)\n","        past_key_values (`Tuple[Tuple[jt.array]]` of length `config.n_layers`):\n","            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n","            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n","            their past given to this model should not be passed as `input_ids` as they have already been computed.\n","        attention_mask (`jt.array` of shape `(batch_size, sequence_length)`, *optional*):\n","            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","\n","            If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n","            `past_key_values`. In other words, the `attention_mask` always has to have the length:\n","            `len(past_key_values) + len(input_ids)`\n","\n","            [What are attention masks?](../glossary#attention-mask)\n","        token_type_ids (`jt.array` of shape `(batch_size, input_ids_length)`, *optional*):\n","            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n","            1]`:\n","\n","            - 0 corresponds to a *sentence A* token,\n","            - 1 corresponds to a *sentence B* token.\n","\n","            [What are token type IDs?](../glossary#token-type-ids)\n","        position_ids (`jt.array` of shape `(batch_size, sequence_length)`, *optional*):\n","            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n","            config.max_position_embeddings - 1]`.\n","\n","            [What are position IDs?](../glossary#position-ids)\n","        head_mask (`jt.array` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n","            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n","\n","            - 1 indicates the head is **not masked**,\n","            - 0 indicates the head is **masked**.\n","\n","        inputs_embeds (`jt.array` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n","            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n","            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n","            model's internal embedding lookup matrix.\n","\n","            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n","            `past_key_values`).\n","        use_cache (`bool`, *optional*):\n","            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n","            `past_key_values`).\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","PARALLELIZE_DOCSTRING = r\"\"\"\n","    This is an experimental feature and is a subject to change at a moment's notice.\n","\n","    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n","    it will evenly distribute blocks across all devices.\n","\n","    Args:\n","        device_map (`Dict[int, list]`, optional, defaults to None):\n","            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n","            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n","            have fewer attention modules mapped to it than other devices. For reference, the gpt2 models have the\n","            following number of attention modules:\n","\n","                - gpt2: 12\n","                - gpt2-medium: 24\n","                - gpt2-large: 36\n","                - gpt2-xl: 48\n","\n","    Example:\n","\n","    ```python\n","    # Here is an example of a device map on a machine with 4 GPUs using gpt2-xl, which has a total of 48 attention modules:\n","    model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n","    device_map = {\n","        0: [0, 1, 2, 3, 4, 5, 6, 7, 8],\n","        1: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n","        2: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n","        3: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n","    }\n","    model.parallelize(device_map)\n","    ```\n","\"\"\"\n","\n","DEPARALLELIZE_DOCSTRING = r\"\"\"\n","    Moves the model to cpu from a model parallel state.\n","\n","    Example:\n","\n","    ```python\n","    # On a 4 GPU machine with gpt2-large:\n","    model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n","    device_map = {\n","        0: [0, 1, 2, 3, 4, 5, 6, 7],\n","        1: [8, 9, 10, 11, 12, 13, 14, 15],\n","        2: [16, 17, 18, 19, 20, 21, 22, 23],\n","        3: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n","    }\n","    model.parallelize(device_map)  # Splits the model across several devices\n","    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n","    ```\n","\"\"\"\n"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["class GPT2Model(GPT2PreTrainedModel):\n","    _keys_to_ignore_on_load_missing = [\"attn.masked_bias\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.embed_dim = config.hidden_size\n","\n","        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n","        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n","\n","        self.drop = nn.Dropout(config.embd_pdrop)\n","        self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n","        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","        self.gradient_checkpointing = False\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n","    def parallelize(self, device_map=None):\n","        # Check validity of device_map\n","        # self.device_map = (\n","        #     get_device_map(len(self.h), range(torch.cuda.device_count())) if device_map is None else device_map\n","        # )\n","        # assert_device_map(self.device_map, len(self.h))\n","        # self.model_parallel = True\n","        # self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n","        # self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n","        # self.wte = self.wte.to(self.first_device)\n","        # self.wpe = self.wpe.to(self.first_device)\n","        # # Load onto devices\n","        # for k, v in self.device_map.items():\n","        #     for block in v:\n","        #         cuda_device = \"cuda:\" + str(k)\n","        #         self.h[block] = self.h[block].to(cuda_device)\n","        # # ln_f to last\n","        # self.ln_f = self.ln_f.to(self.last_device)\n","        pass\n","\n","    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n","    def deparallelize(self):\n","        # self.model_parallel = False\n","        # self.device_map = None\n","        # self.first_device = \"cpu\"\n","        # self.last_device = \"cpu\"\n","        # self.wte = self.wte.to(\"cpu\")\n","        # self.wpe = self.wpe.to(\"cpu\")\n","        # for index in range(len(self.h)):\n","        #     self.h[index] = self.h[index].to(\"cpu\")\n","        # self.ln_f = self.ln_f.to(\"cpu\")\n","        # torch.cuda.empty_cache()\n","        pass\n","\n","    def get_input_embeddings(self):\n","        return self.wte\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.wte = new_embeddings\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.h[layer].attn.prune_heads(heads)\n","\n","    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n","    @add_code_sample_docstrings(\n","        processor_class=_TOKENIZER_FOR_DOC,\n","        checkpoint=_CHECKPOINT_FOR_DOC,\n","        output_type=BaseModelOutputWithPastAndCrossAttentions,\n","        config_class=_CONFIG_FOR_DOC,\n","    )\n","    def execute(\n","        self,\n","        input_ids: Optional[jt.array] = None,\n","        past_key_values: Optional[Tuple[Tuple[jt.array]]] = None,\n","        attention_mask: Optional[jt.array] = None,\n","        token_type_ids: Optional[jt.array] = None,\n","        position_ids: Optional[jt.array] = None,\n","        head_mask: Optional[jt.array] = None,\n","        inputs_embeds: Optional[jt.array] = None,\n","        encoder_hidden_states: Optional[jt.array] = None,\n","        encoder_attention_mask: Optional[jt.array] = None,\n","        use_cache: Optional[bool] = True,\n","        output_attentions: Optional[bool] = True,\n","        output_hidden_states: Optional[bool] = True,\n","        return_dict: Optional[bool] = True,\n","    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = input_ids.size()\n","            input_ids = input_ids.view(-1, input_shape[-1])\n","            batch_size = input_ids.shape[0]\n","        elif inputs_embeds is not None:\n","            input_shape = inputs_embeds.size()[:-1]\n","            batch_size = inputs_embeds.shape[0]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        # device = input_ids.device if input_ids is not None else inputs_embeds.device\n","\n","        if token_type_ids is not None:\n","            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n","        if position_ids is not None:\n","            position_ids = position_ids.view(-1, input_shape[-1])\n","\n","        if past_key_values is None:\n","            past_length = 0\n","            past_key_values = tuple([None] * len(self.h))\n","        else:\n","            past_length = past_key_values[0][0].size(-2)\n","        if position_ids is None:\n","            position_ids = jt.arange(past_length, input_shape[-1] + past_length, dtype=jt.int64)\n","            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n","\n","        # GPT2Attention mask.\n","        if attention_mask is not None:\n","            if batch_size <= 0:\n","                raise ValueError(\"batch_size has to be defined and > 0\")\n","            attention_mask = attention_mask.view(batch_size, -1)\n","            # We create a 3D attention mask from a 2D tensor mask.\n","            # Sizes are [batch_size, 1, 1, to_seq_length]\n","            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","            # this attention mask is more simple than the triangular masking of causal attention\n","            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","            attention_mask = attention_mask[:, None, None, :]\n","\n","            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","            # masked positions, this operation will create a tensor which is 0.0 for\n","            # positions we want to attend and the dtype's smallest value for masked positions.\n","            # Since we are adding it to the raw scores before the softmax, this is\n","            # effectively the same as removing these entirely.\n","\n","            # attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n","            attention_mask = (1.0 - attention_mask) * (-1e32)\n","\n","        # If a 2D or 3D attention mask is provided for the cross-attention\n","        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","        if self.config.add_cross_attention and encoder_hidden_states is not None:\n","            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n","            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n","            if encoder_attention_mask is None:\n","                encoder_attention_mask = jt.ones(encoder_hidden_shape)\n","            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","        else:\n","            encoder_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # head_mask has shape n_layer x batch x n_heads x N x N\n","        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.wte(input_ids)\n","        position_embeds = self.wpe(position_ids)\n","        hidden_states = inputs_embeds + position_embeds\n","\n","        if token_type_ids is not None:\n","            token_type_embeds = self.wte(token_type_ids)\n","            hidden_states = hidden_states + token_type_embeds\n","\n","        hidden_states = self.drop(hidden_states)\n","\n","        output_shape = input_shape + (hidden_states.size(-1),)\n","\n","        presents = () if use_cache else None\n","        all_self_attentions = () if output_attentions else None\n","        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n","        all_hidden_states = () if output_hidden_states else None\n","        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n","            # Model parallel\n","            if self.model_parallel:\n","                pass\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","            current_layer = i\n","\n","            # if self.gradient_checkpointing and self.training:\n","\n","            #     if use_cache:\n","            #         logger.warning(\n","            #             \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n","            #         )\n","            #         use_cache = False\n","\n","            #     def create_custom_forward(module):\n","            #         def custom_forward(*inputs):\n","            #             # None for past_key_value\n","            #             return module(*inputs, use_cache, output_attentions)\n","\n","            #         return custom_forward\n","\n","            #     outputs = torch.utils.checkpoint.checkpoint(\n","            #         create_custom_forward(block),\n","            #         hidden_states,\n","            #         None,\n","            #         attention_mask,\n","            #         head_mask[i],\n","            #         encoder_hidden_states,\n","            #         encoder_attention_mask,\n","            #     )\n","            # else:\n","            outputs = block(\n","                hidden_states,\n","                layer_past=layer_past,\n","                attention_mask=attention_mask,\n","                head_mask=head_mask[i],\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_attention_mask,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","            )\n","\n","            hidden_states = outputs[0]\n","            if use_cache is True:\n","                presents = presents + (outputs[1],)\n","\n","            if output_attentions:\n","                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n","                if self.config.add_cross_attention:\n","                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n","\n","            # Model Parallel: If it's the last layer for that device, put things on the next device\n","            # if self.model_parallel:\n","            #     for k, v in self.device_map.items():\n","            #         if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n","            #             hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n","\n","        hidden_states = self.ln_f(hidden_states)\n","\n","        hidden_states = hidden_states.view(output_shape)\n","        # Add last hidden state\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","        if not return_dict:\n","            return tuple(\n","                v\n","                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n","                if v is not None\n","            )\n","        return BaseModelOutputWithPastAndCrossAttentions(\n","            last_hidden_state=hidden_states,\n","            past_key_values=presents,\n","            hidden_states=all_hidden_states,\n","            attentions=all_self_attentions,\n","            cross_attentions=all_cross_attentions,\n","        )\n","\n"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["class GPT2LMHeadModel(GPT2PreTrainedModel):\n","    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n","\n","    def __init__(self, config, trans=None, lm_head=None):\n","        super().__init__(config)\n","        if trans:\n","            self.transformer = trans\n","        else:\n","            self.transformer = GPT2Model(config)\n","        if lm_head:\n","            self.lm_head = lm_head\n","        else:\n","            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n","    def parallelize(self, device_map=None):\n","        # self.device_map = (\n","        #     get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n","        #     if device_map is None\n","        #     else device_map\n","        # )\n","        # assert_device_map(self.device_map, len(self.transformer.h))\n","        # self.transformer.parallelize(self.device_map)\n","        # self.lm_head = self.lm_head.to(self.transformer.first_device)\n","        # self.model_parallel = True\n","        pass\n","\n","    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n","    def deparallelize(self):\n","        # self.transformer.deparallelize()\n","        # self.transformer = self.transformer.to(\"cpu\")\n","        # self.lm_head = self.lm_head.to(\"cpu\")\n","        # self.model_parallel = False\n","        # torch.cuda.empty_cache()\n","        pass\n","\n","    def get_output_embeddings(self):\n","        return self.lm_head\n","\n","    def set_output_embeddings(self, new_embeddings):\n","        self.lm_head = new_embeddings\n","\n","    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n","        token_type_ids = kwargs.get(\"token_type_ids\", None)\n","        # only last token for inputs_ids if past is defined in kwargs\n","        if past:\n","            input_ids = input_ids[:, -1].unsqueeze(-1)\n","            if token_type_ids is not None:\n","                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n","\n","        attention_mask = kwargs.get(\"attention_mask\", None)\n","        position_ids = kwargs.get(\"position_ids\", None)\n","\n","        if attention_mask is not None and position_ids is None:\n","            # create position_ids on the fly for batch generation\n","            position_ids = attention_mask.long().cumsum(-1) - 1\n","            position_ids.masked_fill_(attention_mask == 0, 1)\n","            if past:\n","                position_ids = position_ids[:, -1].unsqueeze(-1)\n","        else:\n","            position_ids = None\n","        return {\n","            \"input_ids\": input_ids,\n","            \"past_key_values\": past,\n","            \"use_cache\": kwargs.get(\"use_cache\"),\n","            \"position_ids\": position_ids,\n","            \"attention_mask\": attention_mask,\n","            \"token_type_ids\": token_type_ids,\n","        }\n","\n","    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n","    @add_code_sample_docstrings(\n","        processor_class=_TOKENIZER_FOR_DOC,\n","        checkpoint=_CHECKPOINT_FOR_DOC,\n","        output_type=CausalLMOutputWithCrossAttentions,\n","        config_class=_CONFIG_FOR_DOC,\n","    )\n","    def execute(\n","        self,\n","        input_ids: Optional[jt.array] = None,\n","        past_key_values: Optional[Tuple[Tuple[jt.array]]] = None,\n","        attention_mask: Optional[jt.array] = None,\n","        token_type_ids: Optional[jt.array] = None,\n","        position_ids: Optional[jt.array] = None,\n","        head_mask: Optional[jt.array] = None,\n","        inputs_embeds: Optional[jt.array] = None,\n","        encoder_hidden_states: Optional[jt.array] = None,\n","        encoder_attention_mask: Optional[jt.array] = None,\n","        labels: Optional[jt.array] = None,\n","        use_cache: Optional[bool] = True,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = True,\n","        return_dict: Optional[bool] = True,\n","    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n","        r\"\"\"\n","        labels (`jt.array` of shape `(batch_size, sequence_length)`, *optional*):\n","            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n","            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n","            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            past_key_values=past_key_values,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_attention_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        hidden_states = transformer_outputs[0]\n","\n","        # Set device for model parallelism\n","        # if self.model_parallel:\n","        #     torch.cuda.set_device(self.transformer.first_device)\n","        #     hidden_states = hidden_states.to(self.lm_head.weight.device)\n","\n","        lm_logits = self.lm_head(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            # Shift so that tokens < n predict n\n","            shift_logits = lm_logits[..., :-1, :].contiguous()\n","            shift_labels = labels[..., 1:].contiguous()\n","            # Flatten the tokens\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","\n","        if not return_dict:\n","            output = (lm_logits,) + transformer_outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return CausalLMOutputWithCrossAttentions(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=transformer_outputs.past_key_values,\n","            hidden_states=transformer_outputs.hidden_states,\n","            attentions=transformer_outputs.attentions,\n","            cross_attentions=transformer_outputs.cross_attentions,\n","        )\n","\n","    @staticmethod\n","    def _reorder_cache(past: Tuple[Tuple[jt.array]], beam_idx: jt.array) -> Tuple[Tuple[jt.array]]:\n","        \"\"\"\n","        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n","        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n","        beam_idx at every generation step.\n","        \"\"\"\n","        return tuple(\n","            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n","            for layer_past in past\n","        )\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Part PPLM Script"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","from tqdm import trange\n","# from torchtext import data as torchtext_data\n","# from torchtext import datasets\n","\n","import torch\n","import torch.utils.data as data\n","\n","# from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","import torch\n","import torch.optim\n","import torch.nn.functional as F\n","import numpy as np\n","from IPython import embed\n","from operator import add\n","# from run_gpt2 import top_k_logits\n","import copy\n","import pickle\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","import torch.optim as optim \n","\n","import jittor as jt\n","from style_utils import to_var, top_k_logits\n","from jittor.dataset import Dataset as jtDataset\n","\n","from torchtext import data as torchtext_data\n","from torchtext import datasets\n","from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n","import transformers\n","import random"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["class myClassificationHead(jt.nn.Module):\n","    def __init__(self, class_size=5, embed_size=2048):\n","        super().__init__()\n","        self.class_size = class_size\n","        self.embed_size = embed_size\n","        self.mlp = jt.nn.Linear(embed_size, class_size)\n","    def execute(self, hidden_state):\n","        lm_logits = self.mlp(hidden_state)\n","        return lm_logits\n","\n","class myDiscriminator2mean(jt.nn.Module):\n","    def __init__(self, class_size=5, embed_size=1024, head=None, model=None):\n","        super().__init__()\n","        if head == None:\n","            self.classifierhead = myClassificationHead(class_size=class_size, embed_size=embed_size)\n","        else:\n","            self.classifierhead = head\n","        self.model = model\n","        self.embed_size = embed_size\n","    \n","    def get_classifier(self):\n","        return self.classifierhead\n","\n","    def train_custom(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","        pass\n","        self.classifierhead.train()\n","\n","    def execute(self, x):\n","        mask_src = 1 - x.equal(0).unsqueeze(1).detach()\n","        mask_src = mask_src.repeat(1, self.embed_size, 1) #batch_size, 1024, length (repeat each sentence for 1024 times)\n","\n","        x = x.tolist()\n","        x = jt.array(x,dtype=torch.long)\n","        output_dict = self.model(x, output_hidden_states=True)\n","        hidden = output_dict.hidden_states[-1]\n","        # x = model.forward_embed(x)\n","        # hidden, x = model.forward_transformer_embed(x)\n","        #  Hidden has shape batch_size x length x embed-dim\n","        hidden = hidden.tolist()\n","        hidden = jt.array(hidden)\n","\n","        hidden = hidden.permute(0, 2, 1)\n","        _, _, batch_length = hidden.shape\n","        hidden = hidden * mask_src  # / torch.sum(mask_src, dim=-1).unsqueeze(2).repeat(1, 1, batch_length)\n","        #\n","        hidden = hidden.permute(0, 2, 1)\n","        x =  jt.sum(hidden, dim=1)/(jt.sum(mask_src, dim=-1).detach() + 1e-10)\n","        x = self.classifierhead(x)\n","        x = jt.nn.log_softmax(x, dim=-1)\n","        return x"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["SmallConst = 1e-15"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["def latent_perturb(model, args, context=None, sample=True):\n","    if args.discrim == 'clickbait':\n","        classifier = myClassificationHead(class_size=2, embed_size=1024)\n","        classifier.load_state_dict(torch.load(\"discrim_models/clickbait_classifierhead.pt\"))\n","        classifier.eval()\n","        args.label_class = 1 # clickbaity\n","    #TODO map_location=torch.device('cpu')\n","    elif args.discrim == 'sentiment':\n","        classifier = myClassificationHead(class_size=5, embed_size=1024)\n","        classifier.load_state_dict(torch.load(\"discrim_models/sentiment_classifierhead.pt\",map_location=torch.device('cpu')))\n","        classifier.eval()\n","        if args.label_class < 0:\n","            raise Exception('Wrong class for sentiment, use --label-class 2 for *very positive*, 3 for *very negative*')\n","        #args.label_class = 2 # very pos\n","        #args.label_class = 3 # very neg\n","\n","    elif args.discrim == 'toxicity':\n","        classifier = myClassificationHead(class_size=2, embed_size=1024)\n","        classifier.load_state_dict(torch.load(\"discrim_models/toxicity_classifierhead.pt\"))\n","        classifier.eval()\n","        args.label_class = 0 # not toxic\n","    else:\n","        classifier = None\n","    \n","    # Get tokens for the list of positive words\n","    def list_tokens(word_list):\n","        token_list = []\n","        for word in word_list:\n","            token_list.append(enc.encode(\" \" + word))\n","        return token_list\n","\n","    good_index = []\n","    if args.bag_of_words:\n","        bags_of_words = args.bag_of_words.split(\";\")\n","        for wordlist in bags_of_words:\n","            with open(wordlist, \"r\") as f:\n","                words = f.read()\n","                words = words.split('\\n')\n","            good_index.append(list_tokens(words)) # good_index is the encode of the words\n","        \n","    if args.bag_of_words and classifier:\n","        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n","        args.loss_type = 3\n","\n","    elif args.bag_of_words:\n","        args.loss_type = 1\n","        print('Using PPLM-BoW')\n","\n","    elif classifier is not None:\n","        args.loss_type = 2\n","        print('Using PPLM-Discrim')\n","\n","    else:\n","        raise Exception('Supply either --bag-of-words (-B) or --discrim -D')\n","\n","    original, _, _ = sample_from_hidden(model=model, args=args, context=context,\n","                                  perturb=False, good_index=good_index, classifier=classifier)\n","    \n","    perturbed_list = []\n","    discrim_loss_list = []\n","    loss_in_time_list = []\n","\n","    for i in range(args.num_samples): #num_samples : how many output words\n","        perturbed, discrim_loss, loss_in_time = sample_from_hidden(model=model, args=args, context=context,\n","                                                        perturb=True, good_index=good_index,\n","                                                         classifier=classifier)\n","        perturbed_list.append(perturbed)\n","        if classifier is not None:\n","            discrim_loss_list.append(discrim_loss.data.cpu().numpy())\n","        loss_in_time_list.append(loss_in_time)\n","    \n","    return original, perturbed_list, discrim_loss_list, loss_in_time_list\n"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def sample_from_hidden(model, args, classifier, context=None, past=None,\n","                       sample=True, perturb=True, good_index=None):\n","    \n","    output = jt.int64(context).unsqueeze(0) if context else None\n","    grad_norms = None\n","    loss_in_time = []\n","    for i in trange(args.length, ascii=True):\n","\n","        # Get past/probs for current output, except for last word\n","        # Note that GPT takes 2 inputs: past + current-token\n","        # Therefore, use everything from before current i/p token to generate relevant past\n","\n","        if past is None and output is not None:\n","            prev = output[:, -1:]\n","            res =  model(output[:, :-1])\n","            past = model['past_key_values']\n","\n","            res = model(output)\n","            original_probs, true_past = res['logits'], res['past_key_values']\n","            true_hidden = res['hidden_states']\n","\n","        else:\n","            res = model(output)\n","            original_probs, true_past = res['logits'], res['past_key_values']\n","            true_hidden = res['hidden_states']\n","\n","        # Modify the past if necessary\n","        #TODO\n","        if i >= args.grad_length:\n","            current_stepsize = args.stepsize * 0\n","        else:\n","            current_stepsize = args.stepsize\n","\n","        if not perturb or args.num_iterations == 0:\n","            perturbed_past = past\n","\n","        else:\n","            accumulated_hidden = true_hidden[:, :-1, :] #all hidden states except query word\n","            accumulated_hidden = jt.sum(accumulated_hidden, dim=1)\n","\n","            perturbed_past, _, grad_norms, loss_per_iter = perturb_past(past, model, prev, args,\n","                                                                        good_index=good_index, stepsize=current_stepsize,\n","                                                                        original_probs=original_probs,\n","                                                                        true_past=true_past,\n","                                                                        accumulated_hidden=accumulated_hidden,\n","                                                                        classifier=classifier,\n","                                                                        grad_norms=grad_norms)\n","            loss_in_time.append(loss_per_iter)\n","\n","        res = model(prev, past_key_values=perturbed_past)\n","        test_logits, past = res['logits'], res['past_key_values']\n","         \n","        # test_logits = F.softmax(test_logits[:, -1, :], dim=-1)\n","        # likelywords = torch.topk(test_logits, k=10, dim=-1)\n","        # print(enc.decode(likelywords[1].tolist()[0]))\n","\n","        if classifier is not None:\n","            ce_loss = jt.nn.CrossEntropyLoss()\n","            predicted_sentiment = classifier(jt.mean(true_hidden, dim=1))\n","            label = jt.int64([args.label_class])\n","            true_discrim_loss = ce_loss(predicted_sentiment, label)\n","            print(\"true discrim loss\", true_discrim_loss.data.cpu().numpy())\n","        else:\n","            true_discrim_loss = 0 \n","\n","        hidden = model.hidden_states  # update hidden\n","        logits = model.forward_hidden(hidden)\n","        logits = logits[:, -1, :] / args.temperature  # + SmallConst\n","\n","        # logits = top_k_logits(logits, k=args.top_k)  # + SmallConst\n","\n","        log_probs = jt.nn.softmax(logits, dim=-1)\n","\n","        # Fuse the modified model and original model\n","        if perturb:\n","\n","            # original_probs = top_k_logits(original_probs[:, -1, :]) #+ SmallConst\n","            original_probs = jt.nn.softmax(original_probs[:, -1, :], dim=-1)\n","            # likelywords = jt.topk(original_probs, k=10, dim=-1)\n","            # print(enc.decode(likelywords[1].tolist()[0]))\n","\n","            gm_scale = args.fusion_gm_scale\n","            log_probs = ((log_probs ** gm_scale) * (original_probs ** (1 - gm_scale)))  # + SmallConst\n","            log_probs = top_k_logits(log_probs, k=args.top_k, probs=True)  # + SmallConst\n","\n","            if jt.sum(log_probs) <= 1:\n","                log_probs = log_probs / jt.sum(log_probs)\n","        \n","        else:\n","            logits = top_k_logits(logits, k=args.top_k)  # + SmallConst\n","            log_probs = jt.nn.softmax(logits, dim=-1)\n","\n","        if sample:\n","            # likelywords = jt.topk(log_probs, k=args.top_k, dim=-1)\n","            # print(enc.decode(likelywords[1].tolist()[0]))\n","            # print(likelywords[0].tolist())\n","            # np.random.choice()\n","            prev = jt.multinomial(log_probs, num_samples=1)\n","        else:\n","            _, prev = jt.topk(log_probs, k=1, dim=-1)\n","        # if perturb:\n","        #     prev = future\n","        output = prev if output is None else torch.cat((output, prev), dim=1)  # update output\n","        print(enc.decode(output.tolist()[0]))\n","\n","    return output, true_discrim_loss, loss_in_time\n","\n"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def perturb_past(past, model, prev, args, classifier, good_index=None, stepsize=0.01, vocab_size=50257,\n","                 original_probs=None, accumulated_hidden=None, true_past=None, grad_norms=None):\n","    def key_values_add(x,y):\n","        return list(map(add, x,y))\n","    window_length = args.window_length\n","    gm_scale, kl_scale = args.fusion_gm_scale, args.fusion_kl_scale\n","    one_hot_vectors = []\n","    for good_list in good_index:\n","        good_list = list(filter(lambda x: len(x) <= 1, good_list))\n","        good_list = jt.array(good_list)\n","        num_good = good_list.shape[0]\n","        one_hot_good = jt.zeros(num_good, vocab_size)\n","        one_hot_good.scatter_(1, good_list, 1)\n","        one_hot_vectors.append(one_hot_good)\n","\n","    opti = nn.SGD(model.parameters(), lr=0.0)\n","    # Generate inital perturbed past\n","    past_perturb_orig = []\n","    for p in past:\n","        past_perturb_orig.append([np.random.uniform(0.0,0.0,x.shape).astype('float')] for x in p)\n","\n","    if accumulated_hidden is None:\n","        accumulated_hidden = 0\n","\n","    if args.decay:\n","        decay_mask = jt.arange(0., 1.0 + SmallConst, 1.0/(window_length))[1:]\n","    else:\n","        decay_mask = 1.0\n","\n","    # Generate a mask is gradient perturbated is based on a past window\n","    _, _, current_length, _ = past[0][0].shape\n","\n","    if current_length > window_length and window_length > 0:\n","        ones_key_val_shape = tuple(len(past[0])) + tuple(past[0][0].shape[:-2]) + tuple([window_length]) + tuple(\n","            past[0].shape[-1:]) #(2) + (batch_size, num_heads) + (seq_length) + (n_embd)\n","\n","        zeros_key_val_shape = tuple(len(past[0])) + tuple(past[0][0].shape[:-2]) + tuple([current_length - window_length]) + tuple(\n","            past[0].shape[-1:])\n","\n","        ones_mask = jt.ones(ones_key_val_shape)\n","        ones_mask = decay_mask*ones_mask.permute(0, 1, 2, 4, 3)\n","        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n","\n","        window_mask = jt.concat((ones_mask, jt.zeros(zeros_key_val_shape)), dim=-2)\n","    else:\n","        window_mask = jt.ones_like(past[0])\n","\n","    loss_per_iter = []\n","    #TODO\n","    for i in range(args.num_iterations):\n","        past_perturb = []\n","        for p_ in past_perturb_orig:\n","            past_perturb.append([jt.array(x_) for x_ in p_])\n","        # past_perturb = [jt.array(p_) for p_ in past_perturb_orig]\n","        # past_perturb = [to_var(p_, requires_grad=True) for p_ in past_perturb]\n","\n","        perturbed_past = list(map(add, past, past_perturb))\n","\n","        _, _, current_length, _ = past_perturb[0][0].shape\n","\n","        # Compute hidden using perturbed past\n","        result = model(prev, past_key_values=perturbed_past)\n","        hidden = result['hidden_states']\n","        # _, future_past = model(prev, past_key_values=perturbed_past)\n","        # hidden = model.hidden_states\n","        new_accumulated_hidden = accumulated_hidden + jt.sum(hidden, dim=1)\n","\n","        # TODO: Check the layer-norm consistency of this with trained discriminator\n","        #TODO\n","        logits = result['logits']\n","        logits = logits[:, -1, :]\n","        probabs = jt.nn.softmax(logits, dim=-1)\n","        loss = 0.0\n","        loss_list = []\n","        if args.loss_type == 1 or args.loss_type == 3:\n","            for one_hot_good in one_hot_vectors:\n","                good_logits = jt.matmul(probabs, jt.transpose(one_hot_good))\n","                loss_word = good_logits\n","                loss_word = jt.sum(loss_word)\n","                loss_word = -jt.log(loss_word)\n","                #loss_word = torch.sum(loss_word) /torch.sum(one_hot_good)\n","                loss += loss_word\n","                loss_list.append(loss_word)\n","            print('words', loss)\n","\n","        if args.loss_type == 2 or args.loss_type == 3:\n","            ce_loss = jt.nn.CrossEntropyLoss()#use logits to do several querys, calculate mean value ; no rational explation\n","            new_true_past = true_past\n","            for i in range(args.horizon_length):\n","\n","                future_probabs = jt.nn.softmax(logits, dim=-1)  # Get softmax\n","                future_probabs = jt.unsqueeze(future_probabs, dim=1)\n","\n","                res= model(future_probabs, past_key_values=new_true_past)\n","                new_true_past = res['past_key_values']\n","                future_hidden = res['hidden_states']  # Get expected hidden states\n","                new_accumulated_hidden = new_accumulated_hidden + jt.sum(future_hidden, dim=1)\n","                \n","            predicted_sentiment = classifier(new_accumulated_hidden / (current_length + 1 + args.horizon_length))\n","\n","            label = jt.array([args.label_class], dtype=torch.long)\n","            discrim_loss = ce_loss(predicted_sentiment, label)\n","            print('discrim', discrim_loss)\n","            loss += discrim_loss\n","            loss_list.append(discrim_loss)\n","\n","\n","        kl_loss = 0.0\n","        if kl_scale > 0.0:\n","            p = (jt.nn.softmax(original_probs[:, -1, :], dim=-1))\n","            p = p + SmallConst * (p <= SmallConst)\n","            correction = SmallConst * (probabs <= SmallConst)\n","            corrected_probabs = probabs + correction\n","            kl_loss = kl_scale * ((corrected_probabs * (corrected_probabs / p).log()).sum())\n","            #print('kl_loss', kl_loss.data.cpu().numpy())\n","            loss += kl_loss  # + discrim_loss\n","\n","        print((loss - kl_loss))\n","        \n","        loss_per_iter.append(loss)\n","        #TODO\n","        opti.step(loss)\n","        # loss.backward()\n","        if grad_norms is not None and args.loss_type == 1:\n","            grad_norms = [jt.max(grad_norms[index], jt.norm(p_.grad * window_mask)) for index, p_ in\n","                          enumerate(past_perturb)]\n","        else:\n","            grad_norms = [(jt.norm(p_.grad * window_mask) + SmallConst) for index, p_ in enumerate(past_perturb)]\n","\n","        grad = [#TODO grad\n","            -stepsize * (p_.grad * window_mask / grad_norms[index] ** args.gamma)\n","            for index, p_ in enumerate(past_perturb)]\n","        #TODO\n","        past_perturb_orig = list(map(key_values_add, grad, past_perturb_orig))\n","        #TODO\n","        # for p_ in past_perturb:\n","        #     p_.grad.data.zero_()\n","\n","        new_past = []\n","        for p in past:\n","            new_past.append(p.detach())\n","\n","        past = new_past\n","\n","    past_perturb = []\n","    for p_ in past_perturb_orig:\n","        past_perturb.append([jt.array(x_) for x_ in p_])\n","    # past_perturb = [torch.from_numpy(p_) for p_ in past_perturb_orig]\n","    # past_perturb = [to_var(p_, requires_grad=True) for p_ in past_perturb]\n","    #TODO\n","    perturbed_past = list(map(key_values_add, past, past_perturb))\n","\n","    return perturbed_past, new_accumulated_hidden, grad_norms, loss_per_iter\n"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["enc = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["======================================== Prefix of sentence ========================================\n","<|endoftext|>The lake\n","================================================================================\n","Using PPLM-Discrim\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n"]},{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn [112], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m out1, out_perturb, discrim_loss_list, loss_in_time_list \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_perturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m text_whole \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mdecode(out1\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n","Cell \u001b[0;32mIn [110], line 56\u001b[0m, in \u001b[0;36mlatent_perturb\u001b[0;34m(model, args, context, sample)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupply either --bag-of-words (-B) or --discrim -D\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m original, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgood_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m perturbed_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m discrim_loss_list \u001b[38;5;241m=\u001b[39m []\n","Cell \u001b[0;32mIn [103], line 15\u001b[0m, in \u001b[0;36msample_from_hidden\u001b[0;34m(model, args, classifier, context, past, sample, perturb, good_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     prev \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 15\u001b[0m     _, past \u001b[38;5;241m=\u001b[39m model(output[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     16\u001b[0m     original_probs, true_past \u001b[38;5;241m=\u001b[39m model(output)\n\u001b[1;32m     17\u001b[0m     true_hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhidden_states\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["parser = argparse.ArgumentParser()\n","# \n","parser.add_argument('--model_path', '-M', type=str, default='gpt-2_pt_models/',\n","                    help='')\n","# Bags of words used for PPLM-BoW. Multiple BoWs separated by ;\n","parser.add_argument('--bag-of-words', '-B', type=str, default=None, \n","                    help='')\n","# Discriminator to use for loss-type 2\n","parser.add_argument('--discrim', '-D', type=str, default='sentiment', \n","                    choices=('clickbait', 'sentiment', 'toxicity'), \n","                    help='')\n","parser.add_argument('--label-class', type=int, default=2, help='Class label used for the discriminator')\n","parser.add_argument('--stepsize', type=float, default=0.02)\n","parser.add_argument(\"--length\", type=int, default=100)\n","parser.add_argument(\"--seed\", type=int, default=0)\n","parser.add_argument(\"--temperature\", type=float, default=1.0)\n","# top-k\n","parser.add_argument(\"--top_k\", type=int, default=10)\n","# \n","parser.add_argument(\"--fusion-gm-scale\", type=float, default=0.9)\n","parser.add_argument(\"--fusion-kl-scale\", type=float, default=0.01)\n","parser.add_argument('--nocuda', action='store_true', help='no cuda')\n","# Generate from end-of-text as prefix\n","parser.add_argument('--uncond', action='store_true', help='end-of-text')\n","parser.add_argument(\"--cond-text\", type=str, default='The lake', help='Prefix texts to condition on')\n","parser.add_argument('--num-iterations', type=int, default=3)\n","parser.add_argument('--grad-length', type=int, default=10000)\n","parser.add_argument('--num-samples', type=int, default=1,\n","                    help='Number of samples to generate from the modified latents')\n","parser.add_argument('--horizon-length', type=int, default=1, help='Length of future to optimize over')\n","# parser.add_argument('--force-token', action='store_true', help='no cuda')\n","parser.add_argument('--window-length', type=int, default=0,\n","                    help='Length of past which is being optimizer; 0 corresponds to infinite window length')\n","parser.add_argument('--decay', action='store_true', help='whether to decay or not')\n","parser.add_argument('--gamma', type=float, default=1.5)\n","\n","args = parser.parse_args(args=[])\n","\n","# \n","np.random.seed(args.seed)\n","jt.core.set_seed(args.seed)\n","\n","# use cuda\n","if not args.nocuda: \n","    jt.flags.use_cuda = 0\n","\n","# load_pretrained -- change by Stian\n","# model = GPT2Model()\n","# model.load(args.model_path)\n","config = GPT2Config()\n","mm = GPT2Model(config)\n","mm.load_state_dict(torch.load('pytorch_model.bin'))\n","model = GPT2LMHeadModel(config, mm)\n","\n","# eval\n","model.eval()\n","\n","# \n","# TODO\n","# for param in model.Partermer.item():\n","#     pass\n","\n","if args.uncond:\n","    seq = [[50256, 50256]]\n","\n","else:\n","    # \n","    raw_text = args.cond_text\n","    while not raw_text:\n","        print('Did you forget to add `--cond-text`? ')\n","        raw_text = input(\"Model prompt >>> \")\n","    seq = [[50256] + enc.encode(raw_text)]\n","\n","collect_gen = dict()\n","current_index = 0 \n","for out in seq:\n","\n","    text = enc.decode(out)\n","    print(\"=\" * 40 + \" Prefix of sentence \" + \"=\" * 40)\n","    print(text)\n","    print(\"=\" * 80)\n","\n","    out1, out_perturb, discrim_loss_list, loss_in_time_list = latent_perturb(model=model, args=args, context=out)\n","\n","    text_whole = enc.decode(out1.tolist()[0])\n","\n","    print(\"=\" * 80)\n","    print(\"=\" * 40 + \" Whole sentence (Original)\" + \"=\" * 40)\n","    print(text_whole)\n","    print(\"=\" * 80)\n","\n","    out_perturb_copy = out_perturb\n","\n","    generated = 0\n","    # \n","    for out_perturb in out_perturb_copy:\n","        try:\n","            print(\"=\" * 40 + \" Whole sentence (Perturbed)\" + \"=\" * 40)\n","            text_whole = enc.decode(out_perturb.tolist()[0])\n","            print(text_whole)\n","            print(\"=\" * 80)\n","        except:\n","            pass\n","        collect_gen[current_index] = [out, out_perturb, out1]\n","        # Save the prefix, perturbed seq, original seq for each index\n","\n","        current_index = current_index + 1"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["config = GPT2Config()\n","mm = GPT2Model(config)\n","mm.load_state_dict(torch.load('pytorch_model.bin'))\n","model = GPT2LMHeadModel(config, mm)\n","x = jt.ones((2,10))\n","res=model(x,use_cache=True)"]},{"cell_type":"markdown","metadata":{},"source":["### test hook\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["jt.Var([36.], dtype=float32)\n","jt.Var([[-0.567814    0.17607906 -0.2371229 ]], dtype=float32)\n","jt.Var([[-0.567814    0.17607906 -0.2371229 ]], dtype=float32)\n","jt.Var([[0. 0. 0.]], dtype=float32)\n","[]\n"]},{"data":{"text/plain":["[]"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["my_hook = []\n","def hookfn(x):\n","    my_hook.append(x)\n","class test(nn.Module):\n","    def __init__(self, *args, **kw) -> None:\n","        super().__init__(*args, **kw)\n","        self.ln = nn.Linear(3,1)\n","    def execute(self, x:jt.array) -> None:\n","        # t = self.ln(x)\n","        t = x.sum()\n","        t.register_hook(hookfn)\n","        z = 3*t\n","        z.register_hook(hookfn)\n","        s = 2*z\n","        s.register_hook(hookfn)\n","        return s\n","m = test()\n","x = jt.ones(3)\n","opt = nn.SGD(m.parameters(),lr=0.0)\n","\n","loss = m(x)\n","print(loss)\n","print(m.ln.weight)\n","opt.step(loss)\n","print(m.ln.weight)\n","print((m.ln.weight.opt_grad(opt)))\n","print(my_hook)\n","my_hook = my_hook[::-1]\n","my_hook"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.15 ('pplm')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"54f0cfbc081d2dbd9b32bca22e677f6cddd072e682c5c874abb322034cb62c09"}}},"nbformat":4,"nbformat_minor":2}
